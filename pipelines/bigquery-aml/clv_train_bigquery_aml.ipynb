{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Orchestrating BigQuery and AutoML Tables using Kubeflow Pipelines.\n",
    "This notebook demonstrates how to implement and execute a Kubeflow pipline that uses BigQuery for data pre-processing/feature engineering and AutoML Tables for model training. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a lightweight Python component to parametrize BigQuery SQL query.\n",
    "The pipeline utilizes the [**Submitting a query using BigQuery**](https://aihub.cloud.google.com/p/products%2F4700cd7e-2826-4ce9-a1ad-33f4a5bf7433) component from **AI Hub**. The component requires a BigQuery SQL query as one of its inputs. To avoid hardcoding table names and other parameters of the query, a KFP light-weight Python component is defined. The component dynamically generates a SQL query by substituting placeholders in a query template using values passed as input parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_IMAGE = 'gcr.io/clv-pipelines/base-image:latest'\n",
    "@kfp.dsl.python_component(name='Prepare feature engineering query', base_image=BASE_IMAGE,target_component_file='clean_op.yaml')\n",
    "def prepare_feature_engineering_query(\n",
    "    project_id: str,\n",
    "    source_table_id: str,\n",
    "    threshold_date: str,\n",
    "    predict_end: str,\n",
    "    max_monetary: str,\n",
    "    query_template_uri: str) -> str:\n",
    "    \"\"\"Creates a feature engineering query\"\"\"\n",
    "\n",
    "    from google.cloud import storage\n",
    "    import re\n",
    "    \n",
    "    # Read a query template from GCS\n",
    "    _, bucket, blob_name = re.split(\"gs://|/\", query_template_uri, 2)\n",
    "    blob = storage.Client(project_id).get_bucket(bucket).blob(blob_name)\n",
    "    query_template = blob.download_as_string().decode('utf-8')\n",
    "\n",
    "    # Substitute placeholders in the query template\n",
    "    query = query_template.format(\n",
    "        data_source_id=source_table_id,\n",
    "        threshold_date=threshold_date,\n",
    "        predict_end=predict_end,\n",
    "        max_monetary=max_monetary\n",
    "    )\n",
    "    \n",
    "    return query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "import kfp.gcp as gcp\n",
    "\n",
    "QUERY_TEMPLATE_URI = 'gs://sandbox-235500/sql-templates/create_features_template.sql'\n",
    "BIGQUERY_COMPONENT_SPEC_URI = 'https://raw.githubusercontent.com/kubeflow/pipelines/3b938d664de35db9401c6d198439394a9fca95fa/components/gcp/bigquery/query/component.yaml'\n",
    "AML_IMPORT_DATASET_SPEC_URI = 'https://raw.githubusercontent.com/jarokaz/CLVPipelines/master/components/automl_tables/aml-import-dataset.yaml'\n",
    "AML_TRAIN_MODEL_SPEC_URI = 'https://raw.githubusercontent.com/jarokaz/CLVPipelines/master/components/automl_tables/aml-train-model.yaml'\n",
    "\n",
    "@kfp.dsl.pipeline(\n",
    "    name='CLVTrainingPipeline',\n",
    "    description='CLV Training Pipeline'\n",
    ")\n",
    "def clv_pipeline(\n",
    "    project_id='', \n",
    "    source_table_id='',\n",
    "    features_dataset_id='', \n",
    "    features_table_id='',\n",
    "    features_dataset_location='US',\n",
    "    threshold_date='',\n",
    "    predict_end='',\n",
    "    max_monetary=15000,\n",
    "    automl_compute_region='us-central1',\n",
    "    automl_dataset_name='clv_features',\n",
    "    model_name='clv_regression',\n",
    "    train_budget='1000',\n",
    "    target_column_name='target_monetary',\n",
    "    features_to_exclude='customer_id'\n",
    "):\n",
    "    # Create component factories\n",
    "    prepare_feature_engineering_query_op = kfp.components.func_to_container_op(prepare_feature_engineering_query)\n",
    "    engineer_features_op = kfp.components.load_component_from_url(BIGQUERY_COMPONENT_SPEC_URI)\n",
    "    import_dataset_op = kfp.components.load_component_from_url(AML_IMPORT_DATASET_SPEC_URI)\n",
    "    train_model_op = kfp.components.load_component_from_url(AML_TRAIN_MODEL_SPEC_URI)\n",
    "\n",
    "    # Define the pipeline\n",
    "    prepare_feature_engineering_query_task = prepare_feature_engineering_query_op(\n",
    "        project_id=project_id,\n",
    "        source_table_id=source_table_id,\n",
    "        threshold_date=threshold_date,\n",
    "        predict_end=predict_end,\n",
    "        max_monetary=max_monetary,\n",
    "        query_template_uri=QUERY_TEMPLATE_URI\n",
    "    )\n",
    "\n",
    "    engineer_features_task = engineer_features_op(\n",
    "        query=prepare_feature_engineering_query_task.output,\n",
    "        project_id=project_id,\n",
    "        dataset_id=features_dataset_id,\n",
    "        table_id=features_table_id,\n",
    "        output_gcs_path='',\n",
    "        dataset_location=features_dataset_location,\n",
    "        job_config=''\n",
    "    )\n",
    "\n",
    "    import_dataset_task = import_dataset_op(\n",
    "        project_id=project_id,\n",
    "        location=automl_compute_region,\n",
    "        dataset_name=automl_dataset_name,\n",
    "        description='',\n",
    "        source_data_uri='bq://{}.{}.{}'.format(project_id, features_dataset_id, features_table_id),\n",
    "        target_column_name=target_column_name,\n",
    "        weight_column_name='',\n",
    "        ml_use_column_name=''       \n",
    "    )\n",
    "    import_dataset_task.after(engineer_features_task)\n",
    "\n",
    "    train_model_task = train_model_op(\n",
    "        project_id=project_id,\n",
    "        location=automl_compute_region,\n",
    "        dataset_id=import_dataset_task.outputs['output_dataset_id'],\n",
    "        model_name='test_model',\n",
    "        train_budget=train_budget,\n",
    "        optimization_objective='MINIMIZE_MAE',\n",
    "        target_name=model_name,\n",
    "        features_to_exclude=features_to_exclude\n",
    "        )\n",
    "\n",
    "pipeline_func = clv_pipeline\n",
    "pipeline_filename = pipeline_func.__name__ + '.tar.gz'\n",
    "\n",
    "kfp.compiler.Compiler().compile(pipeline_func, pipeline_filename) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Experiment link <a href=\"http://localhost:8082/#/experiments/details/33d0a499-f266-438e-bfdb-796485f9571e\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run link <a href=\"http://localhost:8082/#/runs/details/9bc2c7f2-7b76-11e9-b1b5-42010a8a0074\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "arguments = {\n",
    "    'project_id': 'sandbox-235500',\n",
    "    'source_table_id': 'sandbox-235500.CLVDataset.transactions',\n",
    "    'features_dataset_id': 'CLVDataset',\n",
    "    'features_table_id': 'clv_features',\n",
    "    'threshold_date': '2011-08-08',\n",
    "    'predict_end': '2011-12-12',\n",
    "    'max_monetary': '15000'\n",
    "}\n",
    "\n",
    "HOST = 'http://localhost:8082'\n",
    "EXPERIMENT_NAME = 'TEST_EXP'\n",
    "\n",
    "client = kfp.Client(HOST)\n",
    "experiment = client.create_experiment(EXPERIMENT_NAME)\n",
    "\n",
    "#Submit a pipeline run\n",
    "run_name = pipeline_func.__name__ + ' run'\n",
    "run_result = client.run_pipeline(experiment.id, run_name, pipeline_filename, arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
