{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Orchestrating BigQuery and AutoML Tables using Kubeflow Pipelines.\n",
    "This notebook demonstrates how to implement and execute a Kubeflow pipline that uses Dataproc/Spark for data pre-processing/feature engineering and AutoML Tables for model training. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining and compiling the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "\n",
    "CREATE_DATAPROC_SPEC_URI = 'https://raw.githubusercontent.com/kubeflow/pipelines/d2f5cc92a46012b9927209e2aaccab70961582dc/components/gcp/dataproc/create_cluster/component.yaml'\n",
    "DELETE_DATAPROC_SPEC_URI = 'https://raw.githubusercontent.com/kubeflow/pipelines/d2f5cc92a46012b9927209e2aaccab70961582dc/components/gcp/dataproc/delete_cluster/component.yaml' \n",
    "SUBMIT_PYSPARK_JOB_SPEC_URI = 'https://raw.githubusercontent.com/kubeflow/pipelines/d2f5cc92a46012b9927209e2aaccab70961582dc/components/gcp/dataproc/submit_pyspark_job/component.yaml'\n",
    "AML_IMPORT_DATASET_SPEC_URI = 'https://raw.githubusercontent.com/jarokaz/CLVPipelines/master/components/automl_tables/aml-import-dataset.yaml'\n",
    "AML_TRAIN_MODEL_SPEC_URI = 'https://raw.githubusercontent.com/jarokaz/CLVPipelines/master/components/automl_tables/aml-train-model.yaml'\n",
    "CREATE_FEATURES_FILE_URI = 'gs://clv-pipelines-scripts/create_features.py'\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name='CLV Training Pipeline - Dataproc',\n",
    "    description='CLV Training Pipeline using Dataproc/Spark for data preparation'\n",
    ")\n",
    "def clv_pipeline_dataproc_automl(\n",
    "    project_id='', \n",
    "    source_gcs_path='',\n",
    "    output_gcs_path='',\n",
    "    threshold_date='',\n",
    "    predict_end='',\n",
    "    max_monetary=15000,\n",
    "    max_partitions=2,\n",
    "    compute_region='us-central1',\n",
    "    automl_dataset_name='clv_features',\n",
    "    model_name='clv_regression',\n",
    "    train_budget='1000',\n",
    "    target_column_name='target_monetary',\n",
    "    features_to_exclude='customer_id'\n",
    "):\n",
    "\n",
    "    dataproc_create_cluster_op = kfp.components.load_component_from_url(CREATE_DATAPROC_SPEC_URI)    \n",
    "    dataproc_delete_cluster_op = kfp.components.load_component_from_url(DELETE_DATAPROC_SPEC_URI)    \n",
    "    dataproc_submit_pyspark_job_op = kfp.components.load_component_from_url(SUBMIT_PYSPARK_JOB_SPEC_URI)    \n",
    "\n",
    "    args = ('['\n",
    "        '\"--source-gcs-path={}\",'\n",
    "        '\"--output-gcs-path={}\",'\n",
    "        '\"--threshold-date={}\",'\n",
    "        '\"--predict-end={}\",'\n",
    "        '\"--max-monetary={}\",'\n",
    "        '\"--max-partitions={}\",'\n",
    "        ']'\n",
    "    ).format(\n",
    "        source_gcs_path, \n",
    "        output_gcs_path,\n",
    "        threshold_date,\n",
    "        predict_end,\n",
    "        max_monetary,\n",
    "        max_partitions)\n",
    "\n",
    "    dataproc_create_cluster_task = dataproc_create_cluster_op(\n",
    "        project_id=project_id,\n",
    "        region=region,\n",
    "        name='',\n",
    "        name_prefix='',\n",
    "        initialization_actions='',\n",
    "        config_bucket='',\n",
    "        image_version='',\n",
    "        cluster='',\n",
    "        wait_interval='30'\n",
    "    ) \n",
    "\n",
    "    dataproc_submit_pyspark_job_task = dataproc_submit_pyspark_job_op(\n",
    "        project_id=project_id,\n",
    "        region=region,\n",
    "        cluster_name=dataproc_create_cluster_task.output,\n",
    "        main_python_file_uri = CREATE_FEATURES_FILE_URI,\n",
    "        args=args,\n",
    "        pyspark_job='{}',\n",
    "        job='{}',\n",
    "        wait_interval='30'\n",
    "    )\n",
    "\n",
    "    dataproc_delete_cluster_task = dataproc_delete_cluster_op(\n",
    "        project_id=project_id,\n",
    "        region=region,\n",
    "        name=dataproc_create_cluster_task.output\n",
    "    )\n",
    "\n",
    "    dataproc_delete_cluster_task.after(dataproc_submit_pyspark_job_task)\n",
    "    \n",
    "    import_dataset_task = import_dataset_op(\n",
    "        project_id=project_id,\n",
    "        location=region,\n",
    "        dataset_name=automl_dataset_name,\n",
    "        source_data_uri='bq://{}.{}.{}'.format(project_id, features_dataset_id, features_table_id)\n",
    "    )\n",
    "    \n",
    "   \n",
    "\n",
    "\n",
    "pipeline_func = clv_dataproc_pipeline\n",
    "pipeline_filename = pipeline_func.__name__ + '.tar.gz'\n",
    "\n",
    "kfp.compiler.Compiler().compile(pipeline_func, pipeline_filename) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit the pipeline for execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arguments = {\n",
    "    'project_id': 'sandbox-235500',\n",
    "    'source_gcs_path': 'gs://sandbox-235500/clv_sales_transactions',\n",
    "    'output_gcs_path': 'gs://sandbox-235500/clv_training_dataset',\n",
    "    'threshold_date': '2011-08-08',\n",
    "    'predict_end': '2011-12-12' \n",
    "}\n",
    "\n",
    "HOST = 'http://localhost:8082'\n",
    "EXPERIMENT_NAME = 'CLV_TRAINING'\n",
    "\n",
    "client = kfp.Client(HOST)\n",
    "experiment = client.create_experiment(EXPERIMENT_NAME)\n",
    "\n",
    "run_name = pipeline_func.__name__ + ' run'\n",
    "run_result = client.run_pipeline(experiment.id, run_name, pipeline_filename, arguments)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
