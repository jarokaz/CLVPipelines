{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install KubeFlow Pipelines SDK and import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable}  -m pip install https://storage.googleapis.com/ml-pipeline/release/0.1.17/kfp.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.5.3 (default, Sep 27 2018, 17:25:39) \n",
      "[GCC 6.3.0 20170516] on linux\n",
      "Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n",
      ">>> \n",
      "Traceback (most recent call last):\n",
      "  File \"<stdin>\", line 1, in <module>\n",
      "KeyboardInterrupt\n",
      ">>> "
     ]
    }
   ],
   "source": [
    "\n",
    "!{sys.executable} -m pip install https://storage.googleapis.com/ml-pipeline/release/0.1.17/kfp.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "from kfp import compiler\n",
    "import kfp.dsl as dsl\n",
    "import kfp.gcp as gcp\n",
    "import kfp.notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create image to be used by components. \n",
    "Using tensorflow's py3 image as base and installing requiered libraries (automl, storage, pandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = 'gs://jksandbox/pipelinestest/out'\n",
    "PROJECT_NAME = 'sandbox-235500'\n",
    "EF_IMAGE='gcr.io/%s/automltables:dev' % PROJECT_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-04-26 18:58:52:INFO:Checking path: gs://jksandbox/pipelinestest/out...\n",
      "2019-04-26 18:58:52:INFO:Generate build files.\n",
      "2019-04-26 18:58:52:INFO:Start a kaniko job for build.\n",
      "2019-04-26 18:58:52:INFO:Found local kubernetes config. Initialized with kube_config.\n",
      "2019-04-26 18:58:58:INFO:5 seconds: waiting for job to complete\n",
      "2019-04-26 18:59:03:INFO:10 seconds: waiting for job to complete\n",
      "2019-04-26 18:59:08:INFO:15 seconds: waiting for job to complete\n",
      "2019-04-26 18:59:13:INFO:20 seconds: waiting for job to complete\n",
      "2019-04-26 18:59:18:INFO:25 seconds: waiting for job to complete\n",
      "2019-04-26 18:59:23:INFO:30 seconds: waiting for job to complete\n",
      "2019-04-26 18:59:28:INFO:35 seconds: waiting for job to complete\n",
      "2019-04-26 18:59:33:INFO:40 seconds: waiting for job to complete\n",
      "2019-04-26 18:59:38:INFO:45 seconds: waiting for job to complete\n",
      "2019-04-26 18:59:43:INFO:50 seconds: waiting for job to complete\n",
      "2019-04-26 18:59:48:INFO:55 seconds: waiting for job to complete\n",
      "2019-04-26 18:59:53:INFO:60 seconds: waiting for job to complete\n",
      "2019-04-26 18:59:58:INFO:65 seconds: waiting for job to complete\n",
      "2019-04-26 19:00:03:INFO:70 seconds: waiting for job to complete\n",
      "2019-04-26 19:00:09:INFO:76 seconds: waiting for job to complete\n",
      "2019-04-26 19:00:14:INFO:81 seconds: waiting for job to complete\n",
      "2019-04-26 19:00:19:INFO:86 seconds: waiting for job to complete\n",
      "2019-04-26 19:00:19:INFO:Kaniko job complete.\n",
      "2019-04-26 19:00:19:INFO:Build image complete.\n"
     ]
    }
   ],
   "source": [
    "%%docker {EF_IMAGE} {OUTPUT_DIR}\n",
    "FROM tensorflow/tensorflow:latest-py3\n",
    "RUN pip3 install --upgrade pandas\n",
    "RUN pip3 install --upgrade google-cloud-storage\n",
    "RUN pip3 install --upgrade google-cloud-automl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create reusable components for running different steps in AutoML Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-04-26 19:08:32:INFO:Build an image that is based on gcr.io/sandbox-235500/automltables:dev and push the image to gcr.io/sandbox-235500/component-create-dataset:latest\n",
      "2019-04-26 19:08:32:INFO:Checking path: gs://jksandbox/pipelinestest/out...\n",
      "2019-04-26 19:08:32:INFO:Generate entrypoint and serialization codes.\n",
      "2019-04-26 19:08:32:INFO:Generate build files.\n",
      "2019-04-26 19:08:32:INFO:Start a kaniko job for build.\n",
      "2019-04-26 19:08:32:INFO:Found local kubernetes config. Initialized with kube_config.\n",
      "2019-04-26 19:08:38:INFO:5 seconds: waiting for job to complete\n",
      "2019-04-26 19:08:43:INFO:10 seconds: waiting for job to complete\n",
      "2019-04-26 19:08:48:INFO:15 seconds: waiting for job to complete\n",
      "2019-04-26 19:08:53:INFO:20 seconds: waiting for job to complete\n",
      "2019-04-26 19:08:58:INFO:25 seconds: waiting for job to complete\n",
      "2019-04-26 19:09:03:INFO:30 seconds: waiting for job to complete\n",
      "2019-04-26 19:09:08:INFO:35 seconds: waiting for job to complete\n",
      "2019-04-26 19:09:13:INFO:40 seconds: waiting for job to complete\n",
      "2019-04-26 19:09:18:INFO:45 seconds: waiting for job to complete\n",
      "2019-04-26 19:09:23:INFO:50 seconds: waiting for job to complete\n",
      "2019-04-26 19:09:28:INFO:55 seconds: waiting for job to complete\n",
      "2019-04-26 19:09:33:INFO:60 seconds: waiting for job to complete\n",
      "2019-04-26 19:09:38:INFO:65 seconds: waiting for job to complete\n",
      "2019-04-26 19:09:38:INFO:Kaniko job complete.\n",
      "2019-04-26 19:09:39:INFO:Build component complete.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function Create dataset(project_id:str, location:str, display_name:str)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " def create_dataset(\n",
    "    project_id: str,\n",
    "    location: str,\n",
    "    display_name: str) -> str:\n",
    "        \n",
    "    from google.cloud import automl_v1beta1\n",
    "    \n",
    "    client = automl_v1beta1.AutoMlClient()\n",
    "    \n",
    "    location_path = client.location_path(project_id, location)\n",
    "    \n",
    "    create_dataset_response = client.create_dataset(\n",
    "        location_path,\n",
    "        {\n",
    "            'display_name': display_name,\n",
    "            'tables_dataset_metadata': {}})\n",
    "    \n",
    "    return(create_dataset_response.name)\n",
    "    \n",
    "compiler.build_python_component(\n",
    "    component_func = create_dataset,\n",
    "    staging_gcs_path = OUTPUT_DIR,\n",
    "    base_image=EF_IMAGE,\n",
    "    target_component_file='component-create-dataset.yaml',\n",
    "    target_image = 'gcr.io/' + PROJECT_NAME + '/component-create-dataset:latest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-04-26 19:11:31:INFO:Build an image that is based on gcr.io/sandbox-235500/automltables:dev and push the image to gcr.io/sandbox-235500/component-import-data:latest\n",
      "2019-04-26 19:11:31:INFO:Checking path: gs://jksandbox/pipelinestest/out...\n",
      "2019-04-26 19:11:31:INFO:Generate entrypoint and serialization codes.\n",
      "2019-04-26 19:11:31:INFO:Generate build files.\n",
      "2019-04-26 19:11:31:INFO:Start a kaniko job for build.\n",
      "2019-04-26 19:11:31:INFO:Found local kubernetes config. Initialized with kube_config.\n",
      "2019-04-26 19:11:37:INFO:5 seconds: waiting for job to complete\n",
      "2019-04-26 19:11:42:INFO:10 seconds: waiting for job to complete\n",
      "2019-04-26 19:11:47:INFO:15 seconds: waiting for job to complete\n",
      "2019-04-26 19:11:52:INFO:20 seconds: waiting for job to complete\n",
      "2019-04-26 19:11:57:INFO:25 seconds: waiting for job to complete\n",
      "2019-04-26 19:12:02:INFO:30 seconds: waiting for job to complete\n",
      "2019-04-26 19:12:07:INFO:35 seconds: waiting for job to complete\n",
      "2019-04-26 19:12:12:INFO:40 seconds: waiting for job to complete\n",
      "2019-04-26 19:12:17:INFO:45 seconds: waiting for job to complete\n",
      "2019-04-26 19:12:22:INFO:50 seconds: waiting for job to complete\n",
      "2019-04-26 19:12:27:INFO:55 seconds: waiting for job to complete\n",
      "2019-04-26 19:12:32:INFO:60 seconds: waiting for job to complete\n",
      "2019-04-26 19:12:33:INFO:Kaniko job complete.\n",
      "2019-04-26 19:12:33:INFO:Build component complete.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function Import data(dataset_name:str, source:str, input_uri:str)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " def import_data(\n",
    "    dataset_name: str,\n",
    "    source: str,\n",
    "    input_uri: str) -> str:\n",
    "        \n",
    "    from google.cloud import automl_v1beta1\n",
    "    \n",
    "    client = automl_v1beta1.AutoMlClient()\n",
    "    \n",
    "    input_config = {\n",
    "        source: {\n",
    "            'input_uri': input_uri\n",
    "        }}\n",
    "    \n",
    "    import_data_response = client.import_data(\n",
    "        dataset_name,\n",
    "        input_config)\n",
    "    \n",
    "    import_data_response.result()\n",
    "    \n",
    "    return(dataset_name)\n",
    "    \n",
    "compiler.build_python_component(\n",
    "    component_func = import_data,\n",
    "    staging_gcs_path = OUTPUT_DIR,\n",
    "    base_image=EF_IMAGE,\n",
    "    target_component_file='component-import-data.yaml',\n",
    "    target_image = 'gcr.io/' + PROJECT_NAME + '/component-import-data:latest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-04-26 19:12:49:INFO:Build an image that is based on gcr.io/sandbox-235500/automltables:dev and push the image to gcr.io/sandbox-235500/component-column-specs:latest\n",
      "2019-04-26 19:12:49:INFO:Checking path: gs://jksandbox/pipelinestest/out...\n",
      "2019-04-26 19:12:49:INFO:Generate entrypoint and serialization codes.\n",
      "2019-04-26 19:12:49:INFO:Generate build files.\n",
      "2019-04-26 19:12:50:INFO:Start a kaniko job for build.\n",
      "2019-04-26 19:12:50:INFO:Found local kubernetes config. Initialized with kube_config.\n",
      "2019-04-26 19:12:55:INFO:5 seconds: waiting for job to complete\n",
      "2019-04-26 19:13:00:INFO:10 seconds: waiting for job to complete\n",
      "2019-04-26 19:13:05:INFO:15 seconds: waiting for job to complete\n",
      "2019-04-26 19:13:10:INFO:20 seconds: waiting for job to complete\n",
      "2019-04-26 19:13:15:INFO:25 seconds: waiting for job to complete\n",
      "2019-04-26 19:13:20:INFO:30 seconds: waiting for job to complete\n",
      "2019-04-26 19:13:25:INFO:35 seconds: waiting for job to complete\n",
      "2019-04-26 19:13:30:INFO:40 seconds: waiting for job to complete\n",
      "2019-04-26 19:13:35:INFO:45 seconds: waiting for job to complete\n",
      "2019-04-26 19:13:40:INFO:50 seconds: waiting for job to complete\n",
      "2019-04-26 19:13:46:INFO:55 seconds: waiting for job to complete\n",
      "2019-04-26 19:13:51:INFO:60 seconds: waiting for job to complete\n",
      "2019-04-26 19:13:51:INFO:Kaniko job complete.\n",
      "2019-04-26 19:13:51:INFO:Build component complete.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function Column specs(dataset_name:str, bucket:str)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " def column_specs(\n",
    "    dataset_name: str,\n",
    "    bucket: str) -> str:\n",
    "        \n",
    "    from google.cloud import automl_v1beta1\n",
    "    import google.cloud.automl_v1beta1.proto.data_types_pb2 as data_types\n",
    "    from google.cloud import storage\n",
    "    import pickle\n",
    "    import json\n",
    "    import pandas as pd\n",
    "    \n",
    "    client = automl_v1beta1.AutoMlClient()\n",
    "    \n",
    "    list_table_specs_response = client.list_table_specs(dataset_name)\n",
    "    table_specs = [s for s in list_table_specs_response]\n",
    "    table_spec_name = table_specs[0].name\n",
    "    list_column_specs_response = client.list_column_specs(table_spec_name)\n",
    "    column_specs = {s.display_name: s for s in list_column_specs_response}\n",
    "    \n",
    "    file_blob = 'tables/column-specs/' + dataset_name + '.csv'\n",
    "    \n",
    "    ui_out = {\n",
    "        'version': 1,\n",
    "        'outputs': [\n",
    "            {\n",
    "                'type': 'table',\n",
    "                'source': 'gs://'+bucket+'/' + file_blob,\n",
    "                'header': ['Column', 'Type'],\n",
    "                'format':'csv'}]}\n",
    "    \n",
    "    column_types  = pd.DataFrame({\n",
    "        'Column':[x for x in column_specs.keys()],\n",
    "        'Type':[data_types.TypeCode.Name(\n",
    "            column_specs[x].data_type.type_code) for x in column_specs.keys()]\n",
    "    })\n",
    "    \n",
    "    column_types.to_csv('data.csv', header=None, index=None)\n",
    "    gcs_bucket = storage.Client().bucket(bucket)\n",
    "    blob = gcs_bucket.blob(file_blob)\n",
    "    blob.upload_from_filename('data.csv')\n",
    "    \n",
    "    \n",
    "    with open(\n",
    "        '/mlpipeline-ui-metadata.json',\n",
    "        'w') as fp:\n",
    "        json.dump(ui_out, fp)\n",
    "    \n",
    "    \n",
    "    with open(\n",
    "        'data.pk',\n",
    "        mode='wb') as fp:\n",
    "        pickle.dump(column_specs, fp)\n",
    "\n",
    "    \n",
    "    file_pickle = 'tables/column-specs/' + dataset_name + '.pk'\n",
    "    \n",
    "    blob = gcs_bucket.blob(file_pickle)\n",
    "    blob.upload_from_filename('data.pk')\n",
    "    \n",
    "    return(file_pickle)\n",
    "    \n",
    "compiler.build_python_component(\n",
    "    component_func = column_specs,\n",
    "    staging_gcs_path = OUTPUT_DIR,\n",
    "    base_image=EF_IMAGE,\n",
    "    target_component_file='component-column-specs.yaml',\n",
    "    target_image = 'gcr.io/' + PROJECT_NAME + '/component-column-specs:latest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-04-26 19:14:39:INFO:Build an image that is based on gcr.io/sandbox-235500/automltables:dev and push the image to gcr.io/sandbox-235500/component-update-column:latest\n",
      "2019-04-26 19:14:39:INFO:Checking path: gs://jksandbox/pipelinestest/out...\n",
      "2019-04-26 19:14:39:INFO:Generate entrypoint and serialization codes.\n",
      "2019-04-26 19:14:39:INFO:Generate build files.\n",
      "2019-04-26 19:14:40:INFO:Start a kaniko job for build.\n",
      "2019-04-26 19:14:40:INFO:Found local kubernetes config. Initialized with kube_config.\n",
      "2019-04-26 19:14:45:INFO:5 seconds: waiting for job to complete\n",
      "2019-04-26 19:14:50:INFO:10 seconds: waiting for job to complete\n",
      "2019-04-26 19:14:55:INFO:15 seconds: waiting for job to complete\n",
      "2019-04-26 19:15:00:INFO:20 seconds: waiting for job to complete\n",
      "2019-04-26 19:15:05:INFO:25 seconds: waiting for job to complete\n",
      "2019-04-26 19:15:10:INFO:30 seconds: waiting for job to complete\n",
      "2019-04-26 19:15:15:INFO:35 seconds: waiting for job to complete\n",
      "2019-04-26 19:15:20:INFO:40 seconds: waiting for job to complete\n",
      "2019-04-26 19:15:25:INFO:45 seconds: waiting for job to complete\n",
      "2019-04-26 19:15:30:INFO:50 seconds: waiting for job to complete\n",
      "2019-04-26 19:15:36:INFO:55 seconds: waiting for job to complete\n",
      "2019-04-26 19:15:41:INFO:60 seconds: waiting for job to complete\n",
      "2019-04-26 19:15:41:INFO:Kaniko job complete.\n",
      "2019-04-26 19:15:41:INFO:Build component complete.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function Update column(bucket:str, column_specs_file:str, column_name:str, column_type:str)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " def update_column(\n",
    "    bucket: str,\n",
    "    column_specs_file: str,\n",
    "    column_name: str,\n",
    "    column_type: str) -> str:\n",
    "        \n",
    "    from google.cloud import automl_v1beta1\n",
    "    import google.cloud.automl_v1beta1.proto.data_types_pb2 as data_types\n",
    "    from google.cloud import storage\n",
    "    import pickle\n",
    "    import json\n",
    "    import pandas as pd\n",
    "    \n",
    "    gcs_bucket = storage.Client().bucket(bucket)\n",
    "    blob = gcs_bucket.blob(column_specs_file)\n",
    "    \n",
    "    with open(\n",
    "        'specs.pk',\n",
    "        mode='wb')  as fp:\n",
    "        blob.download_to_file(fp)\n",
    "    \n",
    "    with open(\n",
    "        'specs.pk',\n",
    "        mode='rb') as fp:\n",
    "        column_specs = pickle.load(fp)\n",
    "    \n",
    "    client = automl_v1beta1.AutoMlClient()\n",
    "    \n",
    "    \n",
    "    update_column_spec_dict = {\n",
    "        \"name\": column_specs[column_name].name,\n",
    "        \"data_type\": {\n",
    "            \"type_code\": column_type\n",
    "        }\n",
    "    }\n",
    "    column_specs[column_name] = client.update_column_spec(update_column_spec_dict)\n",
    "    \n",
    "    file_blob = 'tables/column-specs/' + column_specs[column_name].name + '.csv'\n",
    "    \n",
    "    ui_out = {\n",
    "        'version': 1,\n",
    "        'outputs': [\n",
    "            {\n",
    "                'type': 'table',\n",
    "                'source': 'gs://'+bucket+'/' + file_blob,\n",
    "                'header': ['Column', 'Type'],\n",
    "                'format':'csv'}]}\n",
    "    \n",
    "    column_types  = pd.DataFrame({\n",
    "        'Column':[x for x in column_specs.keys()],\n",
    "        'Type':[data_types.TypeCode.Name(\n",
    "            column_specs[x].data_type.type_code) for x in column_specs.keys()]\n",
    "    })\n",
    "    \n",
    "    column_types.to_csv('data.csv', header=None, index=None)\n",
    "    blob = gcs_bucket.blob(file_blob)\n",
    "    blob.upload_from_filename('data.csv')\n",
    "    \n",
    "    \n",
    "    with open(\n",
    "        '/mlpipeline-ui-metadata.json',\n",
    "        'w') as fp:\n",
    "        json.dump(ui_out, fp)\n",
    "    \n",
    "    \n",
    "    with open(\n",
    "        'data.pk',\n",
    "        mode='wb') as fp:\n",
    "        pickle.dump(column_specs, fp)\n",
    "\n",
    "    \n",
    "    file_pickle = 'tables/column-specs/' + column_specs[column_name].name + '.pk'\n",
    "    \n",
    "    blob = gcs_bucket.blob(file_pickle)\n",
    "    blob.upload_from_filename('data.pk')\n",
    "    \n",
    "    return(file_pickle)\n",
    "    \n",
    "compiler.build_python_component(\n",
    "    component_func = update_column,\n",
    "    staging_gcs_path = OUTPUT_DIR,\n",
    "    base_image=EF_IMAGE,\n",
    "    target_component_file='component-update-column.yaml',\n",
    "    target_image = 'gcr.io/' + PROJECT_NAME + '/component-update-column:latest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-04-26 19:16:14:INFO:Build an image that is based on gcr.io/sandbox-235500/automltables:dev and push the image to gcr.io/sandbox-235500/component-update-dataset:latest\n",
      "2019-04-26 19:16:14:INFO:Checking path: gs://jksandbox/pipelinestest/out...\n",
      "2019-04-26 19:16:14:INFO:Generate entrypoint and serialization codes.\n",
      "2019-04-26 19:16:14:INFO:Generate build files.\n",
      "2019-04-26 19:16:15:INFO:Start a kaniko job for build.\n",
      "2019-04-26 19:16:15:INFO:Found local kubernetes config. Initialized with kube_config.\n",
      "2019-04-26 19:16:20:INFO:5 seconds: waiting for job to complete\n",
      "2019-04-26 19:16:25:INFO:10 seconds: waiting for job to complete\n",
      "2019-04-26 19:16:30:INFO:15 seconds: waiting for job to complete\n",
      "2019-04-26 19:16:35:INFO:20 seconds: waiting for job to complete\n",
      "2019-04-26 19:16:40:INFO:25 seconds: waiting for job to complete\n",
      "2019-04-26 19:16:45:INFO:30 seconds: waiting for job to complete\n",
      "2019-04-26 19:16:51:INFO:35 seconds: waiting for job to complete\n",
      "2019-04-26 19:16:56:INFO:40 seconds: waiting for job to complete\n",
      "2019-04-26 19:17:01:INFO:45 seconds: waiting for job to complete\n",
      "2019-04-26 19:17:06:INFO:50 seconds: waiting for job to complete\n",
      "2019-04-26 19:17:11:INFO:55 seconds: waiting for job to complete\n",
      "2019-04-26 19:17:16:INFO:60 seconds: waiting for job to complete\n",
      "2019-04-26 19:17:16:INFO:Kaniko job complete.\n",
      "2019-04-26 19:17:16:INFO:Build component complete.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function Update dataset(dataset_name:str, bucket:str, column_specs_file:str, label_column:str, split_column:str)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " def update_dataset(\n",
    "    dataset_name: str,\n",
    "    bucket: str,\n",
    "    column_specs_file: str,\n",
    "    label_column: str,\n",
    "    split_column: str) -> str:\n",
    "        \n",
    "    from google.cloud import automl_v1beta1\n",
    "    from google.cloud import storage\n",
    "    import pickle\n",
    "    \n",
    "    gcs_bucket = storage.Client().bucket(bucket)\n",
    "    blob = gcs_bucket.blob(column_specs_file)\n",
    "    \n",
    "    with open(\n",
    "        'specs.pk',\n",
    "        mode='wb')  as fp:\n",
    "        blob.download_to_file(fp)\n",
    "    \n",
    "    with open(\n",
    "        'specs.pk',\n",
    "        mode='rb') as fp:\n",
    "        column_specs = pickle.load(fp)\n",
    "    \n",
    "    client = automl_v1beta1.AutoMlClient()\n",
    "    \n",
    "    label_column_spec = column_specs[label_column]\n",
    "    label_column_id = label_column_spec.name.rsplit('/', 1)[-1]\n",
    "\n",
    "    split_column_spec = column_specs[split_column]\n",
    "    split_column_id = split_column_spec.name.rsplit('/', 1)[-1]\n",
    "\n",
    "    update_dataset_dict = {\n",
    "        'name': dataset_name,\n",
    "        'tables_dataset_metadata': {\n",
    "            'target_column_spec_id': label_column_id,\n",
    "            'ml_use_column_spec_id': split_column_id,\n",
    "        }\n",
    "    }\n",
    "    client.update_dataset(update_dataset_dict)\n",
    "    \n",
    "    return(column_specs_file)\n",
    "    \n",
    "compiler.build_python_component(\n",
    "    component_func = update_dataset,\n",
    "    staging_gcs_path = OUTPUT_DIR,\n",
    "    base_image=EF_IMAGE,\n",
    "    target_component_file='component-update-dataset.yaml',\n",
    "    target_image = 'gcr.io/' + PROJECT_NAME + '/component-update-dataset:latest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-04-26 19:20:29:INFO:Build an image that is based on gcr.io/sandbox-235500/automltables:dev and push the image to gcr.io/sandbox-235500/component-create-model:latest\n",
      "2019-04-26 19:20:29:INFO:Checking path: gs://jksandbox/pipelinestest/out...\n",
      "2019-04-26 19:20:29:INFO:Generate entrypoint and serialization codes.\n",
      "2019-04-26 19:20:29:INFO:Generate build files.\n",
      "2019-04-26 19:20:29:INFO:Start a kaniko job for build.\n",
      "2019-04-26 19:20:29:INFO:Found local kubernetes config. Initialized with kube_config.\n",
      "2019-04-26 19:20:34:INFO:5 seconds: waiting for job to complete\n",
      "2019-04-26 19:20:40:INFO:10 seconds: waiting for job to complete\n",
      "2019-04-26 19:20:45:INFO:15 seconds: waiting for job to complete\n",
      "2019-04-26 19:20:50:INFO:20 seconds: waiting for job to complete\n",
      "2019-04-26 19:20:55:INFO:25 seconds: waiting for job to complete\n",
      "2019-04-26 19:21:00:INFO:30 seconds: waiting for job to complete\n",
      "2019-04-26 19:21:05:INFO:35 seconds: waiting for job to complete\n",
      "2019-04-26 19:21:10:INFO:40 seconds: waiting for job to complete\n",
      "2019-04-26 19:21:15:INFO:45 seconds: waiting for job to complete\n",
      "2019-04-26 19:21:20:INFO:50 seconds: waiting for job to complete\n",
      "2019-04-26 19:21:25:INFO:55 seconds: waiting for job to complete\n",
      "2019-04-26 19:21:30:INFO:60 seconds: waiting for job to complete\n",
      "2019-04-26 19:21:30:INFO:Kaniko job complete.\n",
      "2019-04-26 19:21:30:INFO:Build component complete.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function Create model(project_id:str, location:str, display_name:str, train_hours:str, optimization_objective:str, columns_to_ignore:str, dataset_name:str, bucket:str, column_specs_file:str, label_column:str, split_column:str)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " def create_model(\n",
    "    project_id: str,\n",
    "    location: str,\n",
    "    display_name: str,\n",
    "    train_hours: str,\n",
    "    optimization_objective: str,\n",
    "    columns_to_ignore: str,\n",
    "    dataset_name: str,\n",
    "    bucket: str,\n",
    "    column_specs_file: str,\n",
    "    label_column: str,\n",
    "    split_column: str) -> str:\n",
    "        \n",
    "    from google.cloud import automl_v1beta1\n",
    "    from google.cloud import storage\n",
    "    import pickle\n",
    "    import json\n",
    "    \n",
    "    gcs_bucket = storage.Client().bucket(bucket)\n",
    "    blob = gcs_bucket.blob(column_specs_file)\n",
    "    \n",
    "    with open(\n",
    "        'specs.pk',\n",
    "        mode='wb')  as fp:\n",
    "        blob.download_to_file(fp)\n",
    "    \n",
    "    with open(\n",
    "        'specs.pk',\n",
    "        mode='rb') as fp:\n",
    "        column_specs = pickle.load(fp)\n",
    "    \n",
    "    client = automl_v1beta1.AutoMlClient()\n",
    "    \n",
    "    location_path = client.location_path(project_id, location)\n",
    "    \n",
    "    feat_list = list(column_specs.keys())\n",
    "    feat_list.remove(label_column)\n",
    "    feat_list.remove(split_column)\n",
    "    for c in json.loads(columns_to_ignore):\n",
    "        feat_list.remove(c)\n",
    "\n",
    "    model_dict = {\n",
    "        'display_name': display_name,\n",
    "        'dataset_id': dataset_name.rsplit('/', 1)[-1],\n",
    "        'tables_model_metadata': {\n",
    "          'train_budget_milli_node_hours':int(train_hours) * 1000,\n",
    "          'optimization_objective': optimization_objective,\n",
    "          'target_column_spec': column_specs[label_column],\n",
    "          'input_feature_column_specs': [\n",
    "                column_specs[x] for x in feat_list]}\n",
    "        }\n",
    "\n",
    "    create_model_response = client.create_model(location_path, model_dict)\n",
    "    create_model_result = create_model_response.result()\n",
    "    return(create_model_result.name)\n",
    "\n",
    "\n",
    "compiler.build_python_component(\n",
    "    component_func = create_model,\n",
    "    staging_gcs_path = OUTPUT_DIR,\n",
    "    base_image=EF_IMAGE,\n",
    "    target_component_file='component-create-model.yaml',\n",
    "    target_image = 'gcr.io/' + PROJECT_NAME + '/component-create-model:latest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-04-26 19:17:16:INFO:Build an image that is based on gcr.io/sandbox-235500/automltables:dev and push the image to gcr.io/sandbox-235500/component-evaluate-model:latest\n",
      "2019-04-26 19:17:16:INFO:Checking path: gs://jksandbox/pipelinestest/out...\n",
      "2019-04-26 19:17:16:INFO:Generate entrypoint and serialization codes.\n",
      "2019-04-26 19:17:16:INFO:Generate build files.\n",
      "2019-04-26 19:17:17:INFO:Start a kaniko job for build.\n",
      "2019-04-26 19:17:17:INFO:Found local kubernetes config. Initialized with kube_config.\n",
      "2019-04-26 19:17:22:INFO:5 seconds: waiting for job to complete\n",
      "2019-04-26 19:17:27:INFO:10 seconds: waiting for job to complete\n",
      "2019-04-26 19:17:32:INFO:15 seconds: waiting for job to complete\n",
      "2019-04-26 19:17:37:INFO:20 seconds: waiting for job to complete\n",
      "2019-04-26 19:17:42:INFO:25 seconds: waiting for job to complete\n",
      "2019-04-26 19:17:47:INFO:30 seconds: waiting for job to complete\n",
      "2019-04-26 19:17:52:INFO:35 seconds: waiting for job to complete\n",
      "2019-04-26 19:17:57:INFO:40 seconds: waiting for job to complete\n",
      "2019-04-26 19:18:03:INFO:45 seconds: waiting for job to complete\n",
      "2019-04-26 19:18:08:INFO:50 seconds: waiting for job to complete\n",
      "2019-04-26 19:18:13:INFO:55 seconds: waiting for job to complete\n",
      "2019-04-26 19:18:18:INFO:60 seconds: waiting for job to complete\n",
      "2019-04-26 19:18:18:INFO:Kaniko job complete.\n",
      "2019-04-26 19:18:18:INFO:Build component complete.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function Evaluate model(model_name:str, bucket:str)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " def evaluate_model(\n",
    "    model_name: str,\n",
    "    bucket: str) -> str:\n",
    "        \n",
    "    from google.cloud import automl_v1beta1\n",
    "    from google.cloud import storage\n",
    "    import pandas as pd\n",
    "    import json\n",
    "    \n",
    "    client = automl_v1beta1.AutoMlClient()\n",
    "    \n",
    "    \n",
    "    file_blob = 'tables/evaluate-model/' + model_name + '.csv'\n",
    "\n",
    "    ui_out = {\n",
    "        'version': 1,\n",
    "        'outputs': [\n",
    "            {\n",
    "                'type': 'table',\n",
    "                'source': 'gs://'+bucket+'/' + file_blob,\n",
    "                'header': ['Feature', 'Importance'],\n",
    "                'format':'csv'}]}\n",
    "\n",
    "    model = client.get_model(model_name)\n",
    "    feature_list = [(\n",
    "        x.feature_importance,\n",
    "        x.column_display_name\n",
    "    ) for x in model.tables_model_metadata.tables_model_column_info]\n",
    "\n",
    "    feature_list.sort(reverse=True)\n",
    "\n",
    "\n",
    "    feature_importance  = pd.DataFrame({\n",
    "        'Feature':[x[1] for x in feature_list],\n",
    "        'Importance':[x[0] for x in feature_list]})\n",
    "\n",
    "    feature_importance.to_csv('data.csv', header=None, index=None)\n",
    "    gcs_bucket = storage.Client().bucket(bucket)\n",
    "    blob = gcs_bucket.blob(file_blob)\n",
    "    blob.upload_from_filename('data.csv')\n",
    "\n",
    "\n",
    "    with open(\n",
    "        '/mlpipeline-ui-metadata.json',\n",
    "        'w') as fp:\n",
    "        json.dump(ui_out, fp)\n",
    "    \n",
    "    \n",
    "    \n",
    "    metrics = [x for x in client.list_model_evaluations(model_name)][-1]\n",
    "    return(metrics.regression_evaluation_metrics)\n",
    "    \n",
    "compiler.build_python_component(\n",
    "    component_func = evaluate_model,\n",
    "    staging_gcs_path = OUTPUT_DIR,\n",
    "    base_image=EF_IMAGE,\n",
    "    target_component_file='component-evaluate-model.yaml',\n",
    "    target_image = 'gcr.io/' + PROJECT_NAME + '/component-evaluate-model:latest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample pipeline (Energy Price Forecasting) that uses the AutoML Tables components to build a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name='AutoML Tables',\n",
    "    description='AutoML Tables Pipeline')\n",
    "def automl_tables_pipeline(\n",
    "    project_id = dsl.PipelineParam(\n",
    "        'project_id',\n",
    "        value='energy-forecasting'),\n",
    "    location = dsl.PipelineParam(\n",
    "        'location',\n",
    "        value='us-central1'),\n",
    "    bucket = dsl.PipelineParam(\n",
    "        'bucket',\n",
    "        value='energy-forecasting'),\n",
    "    dataset_display_name = dsl.PipelineParam(\n",
    "        'dataset_display_name',\n",
    "        value='testdataset'),\n",
    "    data_source = dsl.PipelineParam(\n",
    "        'data_source',\n",
    "        value='bigquery_source'),\n",
    "    data_input_uri = dsl.PipelineParam(\n",
    "        'data_input_uri',\n",
    "        value='bq://energy-forecasting.Energy.automldata'),\n",
    "    column_to_update_name = dsl.PipelineParam(\n",
    "        'column_to_update_name',\n",
    "        value='hour'),\n",
    "    column_to_update_type = dsl.PipelineParam(\n",
    "        'column_to_update_type',\n",
    "        value='CATEGORY'),\n",
    "    label_column = dsl.PipelineParam(\n",
    "        'label_column',\n",
    "        value='price'),\n",
    "    split_column = dsl.PipelineParam(\n",
    "        'split_column',\n",
    "        value='split'),\n",
    "    model_display_name = dsl.PipelineParam(\n",
    "        'model_display_name',\n",
    "        value='testmodel'),\n",
    "    model_train_hours = dsl.PipelineParam(\n",
    "        'model_train_hours',\n",
    "        value='1'),\n",
    "    model_optimization_objective = dsl.PipelineParam(\n",
    "        'model_optimization_objective',\n",
    "        value='MINIMIZE_MAE'),\n",
    "    model_columns_to_ignore = dsl.PipelineParam(\n",
    "        'model_columns_to_ignore',\n",
    "        value='[\"date_utc\"]'),\n",
    "):\n",
    "    \n",
    "    CreateDatasetOp = kfp.components.load_component('component-create-dataset.yaml')\n",
    "    ImportDataOp = kfp.components.load_component('component-import-data.yaml')\n",
    "    ColumnSpecsOp = kfp.components.load_component('component-column-specs.yaml')\n",
    "    UpdateColumnOp = kfp.components.load_component('component-update-column.yaml')\n",
    "    UpdateDatasetOp = kfp.components.load_component('component-update-dataset.yaml')\n",
    "    CreateModelOp = kfp.components.load_component('component-create-model.yaml')\n",
    "    EvaluateModelOp = kfp.components.load_component('component-evaluate-model.yaml')\n",
    "    \n",
    "    cd_op = CreateDatasetOp(\n",
    "        project_id,\n",
    "        location,\n",
    "        dataset_display_name).apply(\n",
    "        gcp.use_gcp_secret('user-gcp-sa'))\n",
    "    \n",
    "    id_op = ImportDataOp(\n",
    "        cd_op.output,\n",
    "        data_source,\n",
    "        data_input_uri).apply(\n",
    "        gcp.use_gcp_secret('user-gcp-sa'))\n",
    "    \n",
    "    cs_op = ColumnSpecsOp(\n",
    "        id_op.output,\n",
    "        bucket).apply(\n",
    "        gcp.use_gcp_secret('user-gcp-sa'))\n",
    "    \n",
    "    uc_op = UpdateColumnOp(\n",
    "        bucket,\n",
    "        cs_op.output,\n",
    "        column_to_update_name,\n",
    "        column_to_update_type).apply(\n",
    "        gcp.use_gcp_secret('user-gcp-sa'))\n",
    "    \n",
    "    ud_op = UpdateDatasetOp(\n",
    "        id_op.output,\n",
    "        bucket,\n",
    "        uc_op.output,\n",
    "        label_column,\n",
    "        split_column).apply(\n",
    "        gcp.use_gcp_secret('user-gcp-sa'))\n",
    "    \n",
    "    cm_op = CreateModelOp(\n",
    "        project_id,\n",
    "        location,\n",
    "        model_display_name,\n",
    "        model_train_hours,\n",
    "        model_optimization_objective,\n",
    "        model_columns_to_ignore,\n",
    "        id_op.output,\n",
    "        bucket,\n",
    "        ud_op.output,\n",
    "        label_column,\n",
    "        split_column).apply(\n",
    "        gcp.use_gcp_secret('user-gcp-sa'))\n",
    "    \n",
    "    em_op = EvaluateModelOp(\n",
    "        cm_op.output,\n",
    "        bucket).apply(\n",
    "        gcp.use_gcp_secret('user-gcp-sa'))\n",
    "    \n",
    "    \n",
    "compiler.Compiler().compile(automl_tables_pipeline, 'automl-tables-pipeline.tar.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kfp",
   "language": "python",
   "name": "kfp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
