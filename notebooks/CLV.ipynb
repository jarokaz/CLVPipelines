{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Orchestrate BigQuery and AutoML tables with Kubeflow pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "import kfp.dsl as dsl\n",
    "import kfp.gcp as gcp\n",
    "import kfp.notebook\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a base image to be used by lightweight components\n",
    "The image is created by Kaniko Kubernetes service. The image contains the libraries required to interface with BigQuery, Storage and AutoML tables services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure a staging directory for Kaniko\n",
    "STAGING_DIR = 'gs://jksandbox/staging'\n",
    "PROJECT_NAME = 'sandbox-235500'\n",
    "# Set the base image name\n",
    "BASE_IMAGE='gcr.io/%s/automltablesbase:dev' % PROJECT_NAME\n",
    "TARGET_IMAGE='gcr.io/%s/automltablescreate:dev' % PROJECT_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jupyter docker magic is used to start a Kaniko job. The magic uses a default Kubernetes config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%docker {BASE_IMAGE} {STAGING_DIR}\n",
    "FROM tensorflow/tensorflow:latest-py3\n",
    "RUN pip3 install --upgrade pandas\n",
    "RUN pip3 install --upgrade google-cloud-storage\n",
    "RUN pip3 install --upgrade google-cloud-automl\n",
    "RUN pip3 install --upgrade google-cloud-bigquery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create python lightweight components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data to Big Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "\n",
    "@dsl.python_component(\n",
    "    name='automml_create_dataset',\n",
    "    description='AutoML create dataset',\n",
    "    base_image=BASE_IMAGE\n",
    ")\n",
    "def automl_import_dataset(\n",
    "    project_id: str,\n",
    "    dataset_id: str,\n",
    "    table_id: str) -> NamedTuple('DatasetInfo', \n",
    "                               [('name', str), \n",
    "                                ('total_rows', int),\n",
    "                                ('total_columns', int)]):\n",
    "    \n",
    "    \n",
    "    print(\"Project ID:\", project_id)\n",
    "    print(\"Dataset ID:\", dataset_id)\n",
    "    print(\"Table ID:\", table_id)\n",
    "    \n",
    "    from collections import namedtuple\n",
    "    result = namedtuple('DatasetInfo', ['name', 'total_rows', 'total_colums'])\n",
    "    return result('test', 1000, 10)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY = 'SELECT * FROM `bigquery-public-data.stackoverflow.posts_questions` LIMIT 10'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "AutomlCreateDatasetOp = kfp.components.func_to_container_op(automl_create_dataset, output_component_file='comp.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-04-29 22:05:16:INFO:Build an image that is based on gcr.io/sandbox-235500/automltablesbase:dev and push the image to gcr.io/sandbox-235500/automltablescreate:dev\n",
      "2019-04-29 22:05:16:INFO:Checking path: gs://jksandbox/staging...\n",
      "2019-04-29 22:05:16:INFO:Generate entrypoint and serialization codes.\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Output type not supported and supported types are [int, float, str, bool]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-40fa06e1b365>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mcomponent_func\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mautoml_create_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mstaging_gcs_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSTAGING_DIR\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     target_image=TARGET_IMAGE)\n\u001b[0m",
      "\u001b[0;32m~/kfp/lib/python3.5/site-packages/kfp/compiler/_component_builder.py\u001b[0m in \u001b[0;36mbuild_python_component\u001b[0;34m(component_func, target_image, base_image, dependency, staging_gcs_path, build_image, timeout, namespace, target_component_file, python_version)\u001b[0m\n\u001b[1;32m    531\u001b[0m     builder.build_image_from_func(component_func, namespace=namespace,\n\u001b[1;32m    532\u001b[0m                                   \u001b[0mbase_image\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbase_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m                                   python_version=python_version, dependency=dependency)\n\u001b[0m\u001b[1;32m    534\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Build component complete.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0m_generate_pythonop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomponent_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_component_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/kfp/lib/python3.5/site-packages/kfp/compiler/_component_builder.py\u001b[0m in \u001b[0;36mbuild_image_from_func\u001b[0;34m(self, component_func, namespace, base_image, timeout, dependency, python_version)\u001b[0m\n\u001b[1;32m    404\u001b[0m       \u001b[0mlocal_python_filepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_build_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_arc_python_filepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m       \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Generate entrypoint and serialization codes.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m       \u001b[0mcomplete_component_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate_entrypoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomponent_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpython_version\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_python_filepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomplete_component_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/kfp/lib/python3.5/site-packages/kfp/compiler/_component_builder.py\u001b[0m in \u001b[0;36m_generate_entrypoint\u001b[0;34m(self, component_func, python_version)\u001b[0m\n\u001b[1;32m    314\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Some input arguments do not contain annotations.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'return'\u001b[0m \u001b[0;32min\u001b[0m  \u001b[0mannotations\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mannotations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'return'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Output type not supported and supported types are [int, float, str, bool]'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m     \u001b[0;31m# inputs is a dictionary with key of argument name and value of type class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;31m# output is a type class, e.g. str and int.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: Output type not supported and supported types are [int, float, str, bool]"
     ]
    }
   ],
   "source": [
    "# The return value \"DeployerOp\" represents a step that can be used directly in a pipeline function\n",
    "#AutomlCreateDatasetOp = kfp.compiler.build_python_component(\n",
    "#    component_func=automl_create_dataset,\n",
    "#    staging_gcs_path=STAGING_DIR,\n",
    "#    target_image=TARGET_IMAGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BqQueryOp = kfp.components.load_component_from_url(\n",
    "    'https://raw.githubusercontent.com/kubeflow/pipelines/e8524eefb138725fc06600d1956da0f4dd477178/components/gcp/bigquery/query/component.yaml')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and run a pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "import json\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name='CLVPipeline',\n",
    "    description='CLV Pipeline'\n",
    ")\n",
    "def clv_pipeline(\n",
    "    project_id='', \n",
    "    dataset_id='', \n",
    "    table_id='', \n",
    "    clean_data_query=clean_data_query,\n",
    "    engineer_features_query=engineer_features_query,\n",
    "    target='',\n",
    "    feature_list='',\n",
    "    dataset_location='US'\n",
    "):\n",
    "    \n",
    "    BqQueryOp = kfp.components.load_component_from_url(\n",
    "        'https://raw.githubusercontent.com/kubeflow/pipelines/e8524eefb138725fc06600d1956da0f4dd477178/components/gcp/bigquery/query/component.yaml')\n",
    "\n",
    "        \n",
    "    clean_data_op = BqQueryOp(\n",
    "        query=clean_data_query, \n",
    "        project_id=project_id, \n",
    "        dataset_id=dataset_id, \n",
    "        table_id=table_id, \n",
    "        output_gcs_path='', \n",
    "        dataset_location=dataset_location, \n",
    "        job_config='').apply(gcp.use_gcp_secret('user-gcp-sa'))\n",
    "    \n",
    "    engineer_features_op = BqQueryOp(\n",
    "        query=clean_data_query, \n",
    "        project_id=project_id, \n",
    "        dataset_id=dataset_id, \n",
    "        table_id=table_id, \n",
    "        output_gcs_path='', \n",
    "        dataset_location=dataset_location, \n",
    "        job_config='').apply(gcp.use_gcp_secret('user-gcp-sa'))\n",
    "    \n",
    "    engineer_features_op.after(clean_data_op)\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "pipeline_func = clv_pipeline\n",
    "pipeline_filename = pipeline_func.__name__ + '.tar.gz'\n",
    "\n",
    "kfp.compiler.Compiler().compile(pipeline_func, pipeline_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit the pipeline for execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLEAN_DATA_QUERY = '''\n",
    "SELECT\n",
    "  customer_id,\n",
    "  order_date,\n",
    "  order_value,\n",
    "  order_qty_articles\n",
    "FROM\n",
    "(\n",
    "  SELECT\n",
    "    CustomerID AS customer_id,\n",
    "    PARSE_DATE(\"%m/%d/%y\", SUBSTR(InvoiceDate, 0, 8)) AS order_date,\n",
    "    ROUND(SUM(UnitPrice * Quantity), 2) AS order_value,\n",
    "    SUM(Quantity) AS order_qty_articles,\n",
    "    (\n",
    "      SELECT\n",
    "        MAX(PARSE_DATE(\"%m/%d/%y\", SUBSTR(InvoiceDate, 0, 8)))\n",
    "      FROM\n",
    "        `sandbox-235500.CLVDataset.data_source` tl\n",
    "      WHERE\n",
    "        tl.CustomerID = t.CustomerID\n",
    "    ) latest_order\n",
    "  FROM\n",
    "    `sandbox-235500.CLVDataset.data_source` t\n",
    "  GROUP BY\n",
    "      CustomerID,\n",
    "      order_date\n",
    ") a\n",
    "\n",
    "INNER JOIN (\n",
    "  -- Only customers with more than one positive order values before threshold.\n",
    "  SELECT\n",
    "    CustomerID\n",
    "  FROM (\n",
    "    -- Customers and how many positive order values  before threshold.\n",
    "    SELECT\n",
    "      CustomerID,\n",
    "      SUM(positive_value) cnt_positive_value\n",
    "    FROM (\n",
    "      -- Customer with whether order was positive or not at each date.\n",
    "      SELECT\n",
    "        CustomerID,\n",
    "        (\n",
    "          CASE\n",
    "            WHEN SUM(UnitPrice * Quantity) > 0 THEN 1\n",
    "            ELSE 0\n",
    "          END ) positive_value\n",
    "      FROM\n",
    "        `sandbox-235500.CLVDataset.data_source`\n",
    "      WHERE\n",
    "        PARSE_DATE(\"%m/%d/%y\", SUBSTR(InvoiceDate, 0, 8)) < DATE(\"<<threshold_date>>\")\n",
    "      GROUP BY\n",
    "        CustomerID,\n",
    "        SUBSTR(InvoiceDate, 0, 8) )\n",
    "    GROUP BY\n",
    "      CustomerID )\n",
    "  WHERE\n",
    "    cnt_positive_value > 1\n",
    "  ) b\n",
    "ON\n",
    "  a.customer_id = b. CustomerID\n",
    "--[START common_clean]\n",
    "WHERE\n",
    "  -- Bought in the past 3 months\n",
    "  DATE_DIFF(DATE(\"<<predict_date>>\"), latest_order, DAY) <= 90\n",
    "  -- Make sure returns are consistent.\n",
    "  AND (\n",
    "    (order_qty_articles > 0 and order_Value > 0) OR\n",
    "    (order_qty_articles < 0 and order_Value < 0)\n",
    "  )\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENGINEER_FEATURES_QUERY = '''\n",
    "SELECT\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specify pipeline argument values\n",
    "DATASET_ID = 'mykfpdataset'\n",
    "\n",
    "arguments = {\n",
    "    'project_id': PROJECT_NAME,\n",
    "    'dataset_id': DATASET_ID\n",
    "}\n",
    "\n",
    "\n",
    "HOST = 'http://localhost:8082/api/v1/namespaces/kubeflow/services/ml-pipeline:8888/proxy'\n",
    "EXPERIMENT_NAME = 'CLV'\n",
    "\n",
    "client = kfp.Client(HOST)\n",
    "experiment = client.create_experiment(EXPERIMENT_NAME)\n",
    "\n",
    "#Submit a pipeline run\n",
    "run_name = pipeline_func.__name__ + ' run'\n",
    "run_result = client.run_pipeline(experiment.id, run_name, pipeline_filename, arguments)\n",
    "print(run_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "KFP",
   "language": "python",
   "name": "kfp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
