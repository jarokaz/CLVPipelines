{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Orchestrate BigQuery and AutoML tables with Kubeflow pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "import kfp.dsl as dsl\n",
    "import kfp.gcp as gcp\n",
    "import kfp.notebook\n",
    "\n",
    "from typing import NamedTuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a base image to be used by lightweight components\n",
    "The image is created by Kaniko Kubernetes service. The image contains the libraries required to interface with BigQuery, Storage and AutoML tables services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure a staging directory for Kaniko\n",
    "STAGING_DIR = 'gs://jksandbox/staging'\n",
    "PROJECT_NAME = 'sandbox-235500'\n",
    "# Set the base image name\n",
    "BASE_IMAGE='gcr.io/%s/automltablesbase:dev' % PROJECT_NAME\n",
    "TARGET_IMAGE='gcr.io/%s/automltablescreate:dev' % PROJECT_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jupyter docker magic is used to start a Kaniko job. The magic uses a default Kubernetes config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%docker {BASE_IMAGE} {STAGING_DIR}\n",
    "FROM tensorflow/tensorflow:latest-py3\n",
    "RUN pip3 install --upgrade pandas\n",
    "RUN pip3 install --upgrade google-cloud-storage\n",
    "RUN pip3 install --upgrade google-cloud-automl\n",
    "RUN pip3 install --upgrade google-cloud-bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_id = 'sandbox-235500'\n",
    "dataset_id = 'CLVDataset'\n",
    "transactions_table_id = 'transactions'\n",
    "threshold_date = '2011-08-08'\n",
    "predict_end = '2011-12-12'\n",
    "max_monetary = '15000'\n",
    "model_name = 'CLVModel'\n",
    "order_summaries_table_id = \"order_summaries\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create python lightweight components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PREPROCESSING_QUERY_TEMPLATE = '''\n",
    "SELECT\n",
    "  a.customer_id,\n",
    "  a.order_date,\n",
    "  a.order_value,\n",
    "  a.order_qty_articles\n",
    "FROM\n",
    "(\n",
    "  SELECT\n",
    "    customer_id,\n",
    "    order_date,\n",
    "    ROUND(SUM(unit_price * quantity), 2) AS order_value,\n",
    "    SUM(quantity) AS order_qty_articles,\n",
    "    (\n",
    "      SELECT\n",
    "        MAX(order_date)\n",
    "      FROM\n",
    "        `<<project_id>>.<<dataset_id>>.<<transactions_table_id>>` tl\n",
    "      WHERE\n",
    "        tl.customer_id = t.customer_id\n",
    "    ) latest_order\n",
    "  FROM\n",
    "    `<<project_id>>.<<dataset_id>>.<<transactions_table_id>>` t\n",
    "  GROUP BY\n",
    "      customer_id,\n",
    "      order_date\n",
    ") a\n",
    "\n",
    "INNER JOIN (\n",
    "  -- Only customers with more than one positive order values before threshold.\n",
    "  SELECT\n",
    "    customer_id\n",
    "  FROM (\n",
    "    -- Customers and how many positive order values  before threshold.\n",
    "    SELECT\n",
    "      customer_id,\n",
    "      SUM(positive_value) cnt_positive_value\n",
    "    FROM (\n",
    "      -- Customer with whether order was positive or not at each date.\n",
    "      SELECT\n",
    "        customer_id,\n",
    "        (\n",
    "          CASE\n",
    "            WHEN SUM(unit_price * quantity) > 0 THEN 1\n",
    "            ELSE 0\n",
    "          END ) positive_value\n",
    "      FROM\n",
    "        `<<project_id>>.<<dataset_id>>.<<transactions_table_id>>`\n",
    "      WHERE\n",
    "        order_date < DATE(\"<<threshold_date>>\")\n",
    "      GROUP BY\n",
    "        customer_id,\n",
    "        order_date)\n",
    "    GROUP BY\n",
    "      customer_id )\n",
    "  WHERE\n",
    "    cnt_positive_value > 1\n",
    "  ) b\n",
    "ON\n",
    "  a.customer_id = b.customer_id\n",
    "--[START common_clean]\n",
    "WHERE\n",
    "  -- Bought in the past 3 months\n",
    "  DATE_DIFF(DATE(\"<<predict_end>>\"), latest_order, DAY) <= 90\n",
    "  -- Make sure returns are consistent.\n",
    "  AND (\n",
    "    (order_qty_articles > 0 and order_Value > 0) OR\n",
    "    (order_qty_articles < 0 and order_Value < 0)\n",
    "  )\n",
    "'''\n",
    "\n",
    "FEATURE_ENGINEERING_QUERY_TEMPLATE = '''\n",
    "SELECT\n",
    "  tf.customer_id,\n",
    "  -- For training period\n",
    "  -- Copying the calculations from Lifetimes where first orders are ignored\n",
    "  -- See https://github.com/CamDavidsonPilon/lifetimes/blob/master/lifetimes/utils.py#L246\n",
    "--[START features_target]\n",
    "  ROUND(tf.monetary, 2) as monetary,\n",
    "  tf.cnt_orders AS frequency,\n",
    "  tf.recency,\n",
    "  tf.T,\n",
    "  ROUND(tf.recency/cnt_orders, 2) AS time_between,\n",
    "  ROUND(tf.avg_basket_value, 2) AS avg_basket_value,\n",
    "  ROUND(tf.avg_basket_size, 2) AS avg_basket_size,\n",
    "  tf.cnt_returns,\n",
    "  -- Target calculated for overall period\n",
    "  ROUND(tt.target_monetary, 2) as target_monetary\n",
    "--[END features_target]\n",
    "FROM\n",
    "  -- This SELECT uses only data before threshold to make features.\n",
    "  (\n",
    "    SELECT\n",
    "      customer_id,\n",
    "      SUM(order_value) AS monetary,\n",
    "      DATE_DIFF(MAX(order_date), MIN(order_date), DAY) AS recency,\n",
    "      DATE_DIFF(DATE('<<threshold_date>>'), MIN(order_date), DAY) AS T,\n",
    "      COUNT(DISTINCT order_date) AS cnt_orders,\n",
    "      AVG(order_qty_articles) avg_basket_size,\n",
    "      AVG(order_value) avg_basket_value,\n",
    "      SUM(CASE\n",
    "          WHEN order_value < 1 THEN 1\n",
    "          ELSE 0 END) AS cnt_returns\n",
    "    FROM\n",
    "      -- Makes the order value = 0 if it is the first one\n",
    "      (\n",
    "        SELECT\n",
    "          a.*,\n",
    "          (CASE\n",
    "              WHEN a.order_date = c.order_date_min THEN 0\n",
    "              ELSE a.order_value END) AS order_value_btyd\n",
    "        FROM\n",
    "          `<<project_id>>.<<dataset_id>>.<<order_summaries_table_id>>` a\n",
    "        INNER JOIN (\n",
    "          SELECT\n",
    "            customer_id,\n",
    "            MIN(order_date) AS order_date_min\n",
    "          FROM\n",
    "            `<<project_id>>.<<dataset_id>>.<<order_summaries_table_id>>`\n",
    "          GROUP BY\n",
    "            customer_id) c\n",
    "        ON\n",
    "          c.customer_id = a.customer_id\n",
    "      )\n",
    "    WHERE\n",
    "      order_date <= DATE('<<threshold_date>>')\n",
    "    GROUP BY\n",
    "      customer_id) tf,\n",
    "\n",
    "  -- This SELECT uses all records to calculate the target (could also use data after threshold )\n",
    "  (\n",
    "    SELECT\n",
    "      customer_id,\n",
    "      SUM(order_value) target_monetary\n",
    "    FROM\n",
    "      `<<project_id>>.<<dataset_id>>.<<order_summaries_table_id>>`\n",
    "      --WHERE order_date > DATE('<<threshold_date>>')\n",
    "    GROUP BY\n",
    "      customer_id) tt\n",
    "WHERE\n",
    "  tf.customer_id = tt.customer_id\n",
    "  AND tf.monetary > 0\n",
    "  AND tf.monetary <= <<max_monetary>>\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import BigQuery table to AutoML Tables dataset component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@kfp.dsl.python_component(name='Import dataset', base_image=BASE_IMAGE,target_component_file='import_op.yaml')\n",
    "def import_dataset(\n",
    "    location: str,\n",
    "    project_id: str,\n",
    "    dataset_id: str,\n",
    "    table_id: str,\n",
    "    display_name: str) -> NamedTuple('DatasetInfo', \n",
    "                               [('automl_dataset_name', str), \n",
    "                                ('automl_dataset_id', str)]):\n",
    "    \"\"\"Creates an AutoML Tables dataset from the data in BigQuery.\"\"\"\n",
    "    \n",
    "    from collections import namedtuple\n",
    "    from google.cloud import automl_v1beta1 as automl\n",
    "    \n",
    "    client = automl.AutoMlClient()\n",
    "    # Create dataset\n",
    "    location_path = client.location_path(project_id, location)\n",
    "    create_dataset_response = client.create_dataset(\n",
    "        location_path,\n",
    "        {\n",
    "            'display_name': display_name,\n",
    "            'tables_dataset_metadata': {}\n",
    "        })\n",
    "    \n",
    "    path = \"bq://{}.{}.{}\".format(project_id, dataset_id, table_id)\n",
    "    input_config = {\"bigquery_source\": {\"input_uri\": path}}\n",
    "    import_data_response = client.import_data(create_dataset_response.name, input_config)\n",
    "    print(\"Initiating import ...\")\n",
    "    # synchronous check of operation status.\n",
    "    print(\"Data imported. {}\".format(import_data_response.result()))\n",
    "    \n",
    "    # Return component outputs\n",
    "    result = namedtuple('DatasetInfo', ['automl_dataset_name', 'automl_dataset_id'])\n",
    "    return result(create_dataset_response.display_name, create_dataset_response.name)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate SQL queries from templates\n",
    "This is a workaround for a bug in BigQuery components that prevents using paramtrized queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@kfp.dsl.python_component(name='Preprocessing query', base_image=BASE_IMAGE,target_component_file='clean_op.yaml')\n",
    "def generate_data_preprocessing_query(\n",
    "    query_template: str,\n",
    "    project_id: str,\n",
    "    dataset_id: str,\n",
    "    transactions_table_id: str,\n",
    "    threshold_date: str,\n",
    "    predict_end: str) -> str:\n",
    "    \"\"\"Parametrizes clean data query\"\"\"\n",
    "    \n",
    "    query = str(query_template)\n",
    "    query = query.replace(\"<<project_id>>\", str(project_id))\n",
    "    query = query.replace(\"<<dataset_id>>\", str(dataset_id))\n",
    "    query = query.replace(\"<<threshold_date>>\", str(threshold_date))\n",
    "    query = query.replace(\"<<predict_end>>\", str(predict_end))\n",
    "    query = query.replace(\"<<transactions_table_id>>\", str(transactions_table_id))\n",
    "    \n",
    "    return query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@kfp.dsl.python_component(name='Feature engineering query', base_image=BASE_IMAGE,target_component_file='features_op.yaml')\n",
    "def generate_feature_engineering_query(\n",
    "    query_template: str,\n",
    "    project_id: str,\n",
    "    dataset_id: str,\n",
    "    order_summaries_table_id: str,\n",
    "    threshold_date: str,\n",
    "    max_monetary: int) -> str:\n",
    "    \"\"\"Parametrizes engineer features queries\"\"\"\n",
    "    \n",
    "    query = str(query_template)\n",
    "    query = query.replace(\"<<project_id>>\", str(project_id))\n",
    "    query = query.replace(\"<<dataset_id>>\", str(dataset_id))\n",
    "    query = query.replace(\"<<threshold_date>>\", str(threshold_date))\n",
    "    query = query.replace(\"<<max_monetary>>\", str(max_monetary))\n",
    "    query = query.replace(\"<<order_summaries_table_id>>\", str(order_summaries_table_id))\n",
    "    \n",
    "    return query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = generate_data_preprocessing_query(\n",
    "    DATA_PREPROCESSING_QUERY_TEMPLATE,\n",
    "    project_id,\n",
    "    dataset_id,\n",
    "    transactions_table_id,\n",
    "    threshold_date,\n",
    "    predict_end\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "\n",
    "client = bigquery.Client()\n",
    "\n",
    "job_config = bigquery.QueryJobConfig()\n",
    "table_ref = client.dataset(dataset_id).table(order_summaries_table_id)\n",
    "job_config.destination = table_ref\n",
    "job_config.create_disposition = bigquery.job.CreateDisposition.CREATE_IF_NEEDED\n",
    "job_config.write_disposition = bigquery.job.WriteDisposition.WRITE_TRUNCATE\n",
    "\n",
    "\n",
    "\n",
    "query_job = client.query(query, location='US', job_config=job_config)  # API request\n",
    "rows = iter(query_job.result())  # Waits for query to finish\n",
    "\n",
    "for _ in range(5):\n",
    "    print(next(rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = generate_feature_engineering_query(\n",
    "    FEATURE_ENGINEERING_QUERY_TEMPLATE,\n",
    "    project_id,\n",
    "    dataset_id,\n",
    "    order_summaries_table_id,\n",
    "    threshold_date,\n",
    "    max_monetary\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "\n",
    "client = bigquery.Client()\n",
    "\n",
    "job_config = bigquery.QueryJobConfig()\n",
    "table_ref = client.dataset(dataset_id).table('features')\n",
    "job_config.destination = table_ref\n",
    "job_config.create_disposition = bigquery.job.CreateDisposition.CREATE_IF_NEEDED\n",
    "job_config.write_disposition = bigquery.job.WriteDisposition.WRITE_TRUNCATE\n",
    "\n",
    "\n",
    "\n",
    "query_job = client.query(query, location='US', job_config=job_config)  # API request\n",
    "rows = iter(query_job.result())  # Waits for query to finish\n",
    "\n",
    "for _ in range(5):\n",
    "    print(next(rows))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and run a pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "import json\n",
    "\n",
    "ORDER_SUMMARIES_TABLE_ID = 'order_summaries'\n",
    "FEATURES_TABLE_ID = 'clv_features'\n",
    "AUTOML_DATASET_NAME = 'CLVFeatures'\n",
    "\n",
    "generate_data_preprocessing_query_op = kfp.components.func_to_container_op(generate_data_preprocessing_query)\n",
    "generate_feature_engineering_query_op = kfp.components.func_to_container_op(generate_feature_engineering_query)\n",
    "import_dataset_op = kfp.components.func_to_container_op(import_dataset)\n",
    "\n",
    "bq_query_op = kfp.components.load_component_from_url(\n",
    "        'https://raw.githubusercontent.com/kubeflow/pipelines/e8524eefb138725fc06600d1956da0f4dd477178/components/gcp/bigquery/query/component.yaml')\n",
    "\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name='CLVPipeline',\n",
    "    description='CLV Pipeline'\n",
    ")\n",
    "def clv_pipeline(\n",
    "    project_id='', \n",
    "    dataset_id='', \n",
    "    transactions_table_id='',\n",
    "    threshold_date='',\n",
    "    predict_end='',\n",
    "    model_name='',\n",
    "    max_monetary=15000,\n",
    "    bq_dataset_location='US',\n",
    "    automl_dataset_location='us-central1'\n",
    "):\n",
    "\n",
    "\n",
    "    generate_data_preprocessing_query_task = generate_data_preprocessing_query_op(\n",
    "        query_template=DATA_PREPROCESSING_QUERY_TEMPLATE,\n",
    "        project_id=project_id,\n",
    "        dataset_id=dataset_id,\n",
    "        transactions_table_id=transactions_table_id,\n",
    "        threshold_date=threshold_date,\n",
    "        predict_end=predict_end\n",
    "        )\n",
    "  \n",
    "    generate_feature_engineering_query_task = generate_feature_engineering_query_op(\n",
    "        query_template=FEATURE_ENGINEERING_QUERY_TEMPLATE,\n",
    "        project_id=project_id,\n",
    "        dataset_id=dataset_id,\n",
    "        order_summaries_table_id=ORDER_SUMMARIES_TABLE_ID,\n",
    "        threshold_date=threshold_date,\n",
    "        max_monetary=max_monetary\n",
    "        )\n",
    "    \n",
    "    clean_data_task = bq_query_op(\n",
    "        query=generate_data_preprocessing_query_task.output, \n",
    "        project_id=project_id, \n",
    "        dataset_id=dataset_id, \n",
    "        table_id=ORDER_SUMMARIES_TABLE_ID, \n",
    "        output_gcs_path='', \n",
    "        dataset_location=bq_dataset_location, \n",
    "        job_config='').apply(gcp.use_gcp_secret('user-gcp-sa'))\n",
    "    clean_data_task.name=\"clean_data\"\n",
    " \n",
    "    engineer_features_task = bq_query_op(\n",
    "        query=generate_feature_engineering_query_task.output, \n",
    "        project_id=project_id, \n",
    "        dataset_id=dataset_id, \n",
    "        table_id=FEATURES_TABLE_ID, \n",
    "        output_gcs_path='', \n",
    "        dataset_location=bq_dataset_location, \n",
    "        job_config='').apply(gcp.use_gcp_secret('user-gcp-sa'))\n",
    "    engineer_features_task.name=\"engineer_features\"\n",
    "    \n",
    "    engineer_features_task.after(clean_data_task)\n",
    "    \n",
    "    import_dataset_task = import_dataset_op(automl_dataset_location, project_id, dataset_id, FEATURES_TABLE_ID, AUTOML_DATASET_NAME)\n",
    "    import_dataset_task.name = \"import-from-bq\"\n",
    "    \n",
    "    import_dataset_task.after(engineer_features_task)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_func = clv_pipeline\n",
    "pipeline_filename = pipeline_func.__name__ + '.tar.gz'\n",
    "\n",
    "kfp.compiler.Compiler().compile(pipeline_func, pipeline_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit the pipeline for execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Experiment link <a href=\"http://localhost:8082/api/v1/namespaces/kubeflow/services/ml-pipeline:8888/proxy/#/experiments/details/467df5ba-949c-4781-8478-48fb3ae77859\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run link <a href=\"http://localhost:8082/api/v1/namespaces/kubeflow/services/ml-pipeline:8888/proxy/#/runs/details/e299fcf8-712c-11e9-bf64-42010a800073\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'created_at': datetime.datetime(2019, 5, 8, 1, 2, tzinfo=tzlocal()),\n",
      " 'description': None,\n",
      " 'error': None,\n",
      " 'finished_at': None,\n",
      " 'id': 'e299fcf8-712c-11e9-bf64-42010a800073',\n",
      " 'metrics': None,\n",
      " 'name': 'clv_pipeline run',\n",
      " 'pipeline_spec': {'parameters': [{'name': 'model-name', 'value': 'CLVModel'},\n",
      "                                  {'name': 'threshold-date',\n",
      "                                   'value': '2011-08-08'},\n",
      "                                  {'name': 'predict-end',\n",
      "                                   'value': '2011-12-12'},\n",
      "                                  {'name': 'max-monetary', 'value': '15000'},\n",
      "                                  {'name': 'project-id',\n",
      "                                   'value': 'sandbox-235500'},\n",
      "                                  {'name': 'dataset-id', 'value': 'CLVDataset'},\n",
      "                                  {'name': 'transactions-table-id',\n",
      "                                   'value': 'transactions'}],\n",
      "                   'pipeline_id': None,\n",
      "                   'pipeline_manifest': None,\n",
      "                   'workflow_manifest': '{\"spec\": {\"volumes\": [{\"name\": '\n",
      "                                        '\"gcp-credentials\", \"secret\": '\n",
      "                                        '{\"secretName\": \"user-gcp-sa\"}}], '\n",
      "                                        '\"serviceAccountName\": '\n",
      "                                        '\"pipeline-runner\", \"templates\": '\n",
      "                                        '[{\"name\": \"clean-data\", \"outputs\": '\n",
      "                                        '{\"parameters\": [{\"name\": '\n",
      "                                        '\"bigquery-query-output-gcs-path\", '\n",
      "                                        '\"valueFrom\": {\"path\": '\n",
      "                                        '\"/tmp/kfp/output/bigquery/query-output-path.txt\"}}], '\n",
      "                                        '\"artifacts\": [{\"name\": '\n",
      "                                        '\"mlpipeline-ui-metadata\", \"path\": '\n",
      "                                        '\"/mlpipeline-ui-metadata.json\", \"s3\": '\n",
      "                                        '{\"insecure\": true, \"accessKeySecret\": '\n",
      "                                        '{\"name\": \"mlpipeline-minio-artifact\", '\n",
      "                                        '\"key\": \"accesskey\"}, \"endpoint\": '\n",
      "                                        '\"minio-service.kubeflow:9000\", '\n",
      "                                        '\"secretKeySecret\": {\"name\": '\n",
      "                                        '\"mlpipeline-minio-artifact\", \"key\": '\n",
      "                                        '\"secretkey\"}, \"key\": '\n",
      "                                        '\"runs/{{workflow.uid}}/{{pod.name}}/mlpipeline-ui-metadata.tgz\", '\n",
      "                                        '\"bucket\": \"mlpipeline\"}}, {\"name\": '\n",
      "                                        '\"mlpipeline-metrics\", \"path\": '\n",
      "                                        '\"/mlpipeline-metrics.json\", \"s3\": '\n",
      "                                        '{\"insecure\": true, \"accessKeySecret\": '\n",
      "                                        '{\"name\": \"mlpipeline-minio-artifact\", '\n",
      "                                        '\"key\": \"accesskey\"}, \"endpoint\": '\n",
      "                                        '\"minio-service.kubeflow:9000\", '\n",
      "                                        '\"secretKeySecret\": {\"name\": '\n",
      "                                        '\"mlpipeline-minio-artifact\", \"key\": '\n",
      "                                        '\"secretkey\"}, \"key\": '\n",
      "                                        '\"runs/{{workflow.uid}}/{{pod.name}}/mlpipeline-metrics.tgz\", '\n",
      "                                        '\"bucket\": \"mlpipeline\"}}]}, \"inputs\": '\n",
      "                                        '{\"parameters\": [{\"name\": '\n",
      "                                        '\"dataset-id\"}, {\"name\": '\n",
      "                                        '\"preprocessing-query-output\"}, '\n",
      "                                        '{\"name\": \"project-id\"}]}, '\n",
      "                                        '\"container\": {\"image\": '\n",
      "                                        '\"gcr.io/ml-pipeline/ml-pipeline-gcp:b0147bdbed9f25212408e0468a475289e80e0406\", '\n",
      "                                        '\"env\": [{\"name\": \"KFP_POD_NAME\", '\n",
      "                                        '\"value\": \"{{pod.name}}\"}, {\"name\": '\n",
      "                                        '\"GOOGLE_APPLICATION_CREDENTIALS\", '\n",
      "                                        '\"value\": '\n",
      "                                        '\"/secret/gcp-credentials/user-gcp-sa.json\"}, '\n",
      "                                        '{\"name\": '\n",
      "                                        '\"CLOUDSDK_AUTH_CREDENTIAL_FILE_OVERRIDE\", '\n",
      "                                        '\"value\": '\n",
      "                                        '\"/secret/gcp-credentials/user-gcp-sa.json\"}], '\n",
      "                                        '\"command\": [], \"volumeMounts\": '\n",
      "                                        '[{\"name\": \"gcp-credentials\", '\n",
      "                                        '\"mountPath\": '\n",
      "                                        '\"/secret/gcp-credentials\"}], \"args\": '\n",
      "                                        '[\"kfp_component.google.bigquery\", '\n",
      "                                        '\"query\", \"--query\", '\n",
      "                                        '\"{{inputs.parameters.preprocessing-query-output}}\", '\n",
      "                                        '\"--project_id\", '\n",
      "                                        '\"{{inputs.parameters.project-id}}\", '\n",
      "                                        '\"--dataset_id\", '\n",
      "                                        '\"{{inputs.parameters.dataset-id}}\", '\n",
      "                                        '\"--table_id\", \"order_summaries\", '\n",
      "                                        '\"--output_gcs_path\", \"\", '\n",
      "                                        '\"--job_config\", \"\"]}}, {\"name\": '\n",
      "                                        '\"clvpipeline\", \"inputs\": '\n",
      "                                        '{\"parameters\": [{\"name\": '\n",
      "                                        '\"automl-dataset-location\"}, {\"name\": '\n",
      "                                        '\"dataset-id\"}, {\"name\": '\n",
      "                                        '\"max-monetary\"}, {\"name\": '\n",
      "                                        '\"predict-end\"}, {\"name\": '\n",
      "                                        '\"project-id\"}, {\"name\": '\n",
      "                                        '\"threshold-date\"}, {\"name\": '\n",
      "                                        '\"transactions-table-id\"}]}, \"dag\": '\n",
      "                                        '{\"tasks\": [{\"name\": \"clean-data\", '\n",
      "                                        '\"dependencies\": '\n",
      "                                        '[\"preprocessing-query\"], \"template\": '\n",
      "                                        '\"clean-data\", \"arguments\": '\n",
      "                                        '{\"parameters\": [{\"name\": '\n",
      "                                        '\"dataset-id\", \"value\": '\n",
      "                                        '\"{{inputs.parameters.dataset-id}}\"}, '\n",
      "                                        '{\"name\": '\n",
      "                                        '\"preprocessing-query-output\", '\n",
      "                                        '\"value\": '\n",
      "                                        '\"{{tasks.preprocessing-query.outputs.parameters.preprocessing-query-output}}\"}, '\n",
      "                                        '{\"name\": \"project-id\", \"value\": '\n",
      "                                        '\"{{inputs.parameters.project-id}}\"}]}}, '\n",
      "                                        '{\"name\": \"engineer-features\", '\n",
      "                                        '\"dependencies\": [\"clean-data\", '\n",
      "                                        '\"feature-engineering-query\"], '\n",
      "                                        '\"template\": \"engineer-features\", '\n",
      "                                        '\"arguments\": {\"parameters\": [{\"name\": '\n",
      "                                        '\"dataset-id\", \"value\": '\n",
      "                                        '\"{{inputs.parameters.dataset-id}}\"}, '\n",
      "                                        '{\"name\": '\n",
      "                                        '\"feature-engineering-query-output\", '\n",
      "                                        '\"value\": '\n",
      "                                        '\"{{tasks.feature-engineering-query.outputs.parameters.feature-engineering-query-output}}\"}, '\n",
      "                                        '{\"name\": \"project-id\", \"value\": '\n",
      "                                        '\"{{inputs.parameters.project-id}}\"}]}}, '\n",
      "                                        '{\"name\": \"feature-engineering-query\", '\n",
      "                                        '\"template\": '\n",
      "                                        '\"feature-engineering-query\", '\n",
      "                                        '\"arguments\": {\"parameters\": [{\"name\": '\n",
      "                                        '\"dataset-id\", \"value\": '\n",
      "                                        '\"{{inputs.parameters.dataset-id}}\"}, '\n",
      "                                        '{\"name\": \"max-monetary\", \"value\": '\n",
      "                                        '\"{{inputs.parameters.max-monetary}}\"}, '\n",
      "                                        '{\"name\": \"project-id\", \"value\": '\n",
      "                                        '\"{{inputs.parameters.project-id}}\"}, '\n",
      "                                        '{\"name\": \"threshold-date\", \"value\": '\n",
      "                                        '\"{{inputs.parameters.threshold-date}}\"}]}}, '\n",
      "                                        '{\"name\": \"import-from-bq\", '\n",
      "                                        '\"dependencies\": '\n",
      "                                        '[\"engineer-features\"], \"template\": '\n",
      "                                        '\"import-from-bq\", \"arguments\": '\n",
      "                                        '{\"parameters\": [{\"name\": '\n",
      "                                        '\"automl-dataset-location\", \"value\": '\n",
      "                                        '\"{{inputs.parameters.automl-dataset-location}}\"}, '\n",
      "                                        '{\"name\": \"dataset-id\", \"value\": '\n",
      "                                        '\"{{inputs.parameters.dataset-id}}\"}, '\n",
      "                                        '{\"name\": \"project-id\", \"value\": '\n",
      "                                        '\"{{inputs.parameters.project-id}}\"}]}}, '\n",
      "                                        '{\"name\": \"preprocessing-query\", '\n",
      "                                        '\"template\": \"preprocessing-query\", '\n",
      "                                        '\"arguments\": {\"parameters\": [{\"name\": '\n",
      "                                        '\"dataset-id\", \"value\": '\n",
      "                                        '\"{{inputs.parameters.dataset-id}}\"}, '\n",
      "                                        '{\"name\": \"predict-end\", \"value\": '\n",
      "                                        '\"{{inputs.parameters.predict-end}}\"}, '\n",
      "                                        '{\"name\": \"project-id\", \"value\": '\n",
      "                                        '\"{{inputs.parameters.project-id}}\"}, '\n",
      "                                        '{\"name\": \"threshold-date\", \"value\": '\n",
      "                                        '\"{{inputs.parameters.threshold-date}}\"}, '\n",
      "                                        '{\"name\": \"transactions-table-id\", '\n",
      "                                        '\"value\": '\n",
      "                                        '\"{{inputs.parameters.transactions-table-id}}\"}]}}]}}, '\n",
      "                                        '{\"name\": \"engineer-features\", '\n",
      "                                        '\"outputs\": {\"parameters\": [{\"name\": '\n",
      "                                        '\"bigquery-query-2-output-gcs-path\", '\n",
      "                                        '\"valueFrom\": {\"path\": '\n",
      "                                        '\"/tmp/kfp/output/bigquery/query-output-path.txt\"}}], '\n",
      "                                        '\"artifacts\": [{\"name\": '\n",
      "                                        '\"mlpipeline-ui-metadata\", \"path\": '\n",
      "                                        '\"/mlpipeline-ui-metadata.json\", \"s3\": '\n",
      "                                        '{\"insecure\": true, \"accessKeySecret\": '\n",
      "                                        '{\"name\": \"mlpipeline-minio-artifact\", '\n",
      "                                        '\"key\": \"accesskey\"}, \"endpoint\": '\n",
      "                                        '\"minio-service.kubeflow:9000\", '\n",
      "                                        '\"secretKeySecret\": {\"name\": '\n",
      "                                        '\"mlpipeline-minio-artifact\", \"key\": '\n",
      "                                        '\"secretkey\"}, \"key\": '\n",
      "                                        '\"runs/{{workflow.uid}}/{{pod.name}}/mlpipeline-ui-metadata.tgz\", '\n",
      "                                        '\"bucket\": \"mlpipeline\"}}, {\"name\": '\n",
      "                                        '\"mlpipeline-metrics\", \"path\": '\n",
      "                                        '\"/mlpipeline-metrics.json\", \"s3\": '\n",
      "                                        '{\"insecure\": true, \"accessKeySecret\": '\n",
      "                                        '{\"name\": \"mlpipeline-minio-artifact\", '\n",
      "                                        '\"key\": \"accesskey\"}, \"endpoint\": '\n",
      "                                        '\"minio-service.kubeflow:9000\", '\n",
      "                                        '\"secretKeySecret\": {\"name\": '\n",
      "                                        '\"mlpipeline-minio-artifact\", \"key\": '\n",
      "                                        '\"secretkey\"}, \"key\": '\n",
      "                                        '\"runs/{{workflow.uid}}/{{pod.name}}/mlpipeline-metrics.tgz\", '\n",
      "                                        '\"bucket\": \"mlpipeline\"}}]}, \"inputs\": '\n",
      "                                        '{\"parameters\": [{\"name\": '\n",
      "                                        '\"dataset-id\"}, {\"name\": '\n",
      "                                        '\"feature-engineering-query-output\"}, '\n",
      "                                        '{\"name\": \"project-id\"}]}, '\n",
      "                                        '\"container\": {\"image\": '\n",
      "                                        '\"gcr.io/ml-pipeline/ml-pipeline-gcp:b0147bdbed9f25212408e0468a475289e80e0406\", '\n",
      "                                        '\"env\": [{\"name\": \"KFP_POD_NAME\", '\n",
      "                                        '\"value\": \"{{pod.name}}\"}, {\"name\": '\n",
      "                                        '\"GOOGLE_APPLICATION_CREDENTIALS\", '\n",
      "                                        '\"value\": '\n",
      "                                        '\"/secret/gcp-credentials/user-gcp-sa.json\"}, '\n",
      "                                        '{\"name\": '\n",
      "                                        '\"CLOUDSDK_AUTH_CREDENTIAL_FILE_OVERRIDE\", '\n",
      "                                        '\"value\": '\n",
      "                                        '\"/secret/gcp-credentials/user-gcp-sa.json\"}], '\n",
      "                                        '\"command\": [], \"volumeMounts\": '\n",
      "                                        '[{\"name\": \"gcp-credentials\", '\n",
      "                                        '\"mountPath\": '\n",
      "                                        '\"/secret/gcp-credentials\"}], \"args\": '\n",
      "                                        '[\"kfp_component.google.bigquery\", '\n",
      "                                        '\"query\", \"--query\", '\n",
      "                                        '\"{{inputs.parameters.feature-engineering-query-output}}\", '\n",
      "                                        '\"--project_id\", '\n",
      "                                        '\"{{inputs.parameters.project-id}}\", '\n",
      "                                        '\"--dataset_id\", '\n",
      "                                        '\"{{inputs.parameters.dataset-id}}\", '\n",
      "                                        '\"--table_id\", \"clv_features\", '\n",
      "                                        '\"--output_gcs_path\", \"\", '\n",
      "                                        '\"--job_config\", \"\"]}}, {\"name\": '\n",
      "                                        '\"feature-engineering-query\", '\n",
      "                                        '\"outputs\": {\"parameters\": [{\"name\": '\n",
      "                                        '\"feature-engineering-query-output\", '\n",
      "                                        '\"valueFrom\": {\"path\": '\n",
      "                                        '\"/outputs/Output/data\"}}], '\n",
      "                                        '\"artifacts\": [{\"name\": '\n",
      "                                        '\"mlpipeline-ui-metadata\", \"path\": '\n",
      "                                        '\"/mlpipeline-ui-metadata.json\", \"s3\": '\n",
      "                                        '{\"insecure\": true, \"accessKeySecret\": '\n",
      "                                        '{\"name\": \"mlpipeline-minio-artifact\", '\n",
      "                                        '\"key\": \"accesskey\"}, \"endpoint\": '\n",
      "                                        '\"minio-service.kubeflow:9000\", '\n",
      "                                        '\"secretKeySecret\": {\"name\": '\n",
      "                                        '\"mlpipeline-minio-artifact\", \"key\": '\n",
      "                                        '\"secretkey\"}, \"key\": '\n",
      "                                        '\"runs/{{workflow.uid}}/{{pod.name}}/mlpipeline-ui-metadata.tgz\", '\n",
      "                                        '\"bucket\": \"mlpipeline\"}}, {\"name\": '\n",
      "                                        '\"mlpipeline-metrics\", \"path\": '\n",
      "                                        '\"/mlpipeline-metrics.json\", \"s3\": '\n",
      "                                        '{\"insecure\": true, \"accessKeySecret\": '\n",
      "                                        '{\"name\": \"mlpipeline-minio-artifact\", '\n",
      "                                        '\"key\": \"accesskey\"}, \"endpoint\": '\n",
      "                                        '\"minio-service.kubeflow:9000\", '\n",
      "                                        '\"secretKeySecret\": {\"name\": '\n",
      "                                        '\"mlpipeline-minio-artifact\", \"key\": '\n",
      "                                        '\"secretkey\"}, \"key\": '\n",
      "                                        '\"runs/{{workflow.uid}}/{{pod.name}}/mlpipeline-metrics.tgz\", '\n",
      "                                        '\"bucket\": \"mlpipeline\"}}]}, \"inputs\": '\n",
      "                                        '{\"parameters\": [{\"name\": '\n",
      "                                        '\"dataset-id\"}, {\"name\": '\n",
      "                                        '\"max-monetary\"}, {\"name\": '\n",
      "                                        '\"project-id\"}, {\"name\": '\n",
      "                                        '\"threshold-date\"}]}, \"container\": '\n",
      "                                        '{\"image\": '\n",
      "                                        '\"gcr.io/sandbox-235500/automltablesbase:dev\", '\n",
      "                                        '\"command\": [\"python3\", \"-c\", \"def '\n",
      "                                        'generate_feature_engineering_query(\\\\n    '\n",
      "                                        'query_template: str,\\\\n    '\n",
      "                                        'project_id: str,\\\\n    dataset_id: '\n",
      "                                        'str,\\\\n    order_summaries_table_id: '\n",
      "                                        'str,\\\\n    threshold_date: str,\\\\n    '\n",
      "                                        'max_monetary: int) -> str:\\\\n    '\n",
      "                                        '\\\\\"\\\\\"\\\\\"Parametrizes engineer '\n",
      "                                        'features queries\\\\\"\\\\\"\\\\\"\\\\n    '\n",
      "                                        '\\\\n    query = '\n",
      "                                        'str(query_template)\\\\n    query = '\n",
      "                                        'query.replace(\\\\\"<<project_id>>\\\\\", '\n",
      "                                        'str(project_id))\\\\n    query = '\n",
      "                                        'query.replace(\\\\\"<<dataset_id>>\\\\\", '\n",
      "                                        'str(dataset_id))\\\\n    query = '\n",
      "                                        'query.replace(\\\\\"<<threshold_date>>\\\\\", '\n",
      "                                        'str(threshold_date))\\\\n    query = '\n",
      "                                        'query.replace(\\\\\"<<max_monetary>>\\\\\", '\n",
      "                                        'str(max_monetary))\\\\n    query = '\n",
      "                                        'query.replace(\\\\\"<<order_summaries_table_id>>\\\\\", '\n",
      "                                        'str(order_summaries_table_id))\\\\n    '\n",
      "                                        '\\\\n    return query\\\\n\\\\nimport '\n",
      "                                        'sys\\\\n_args = {\\\\n    '\n",
      "                                        \"'query_template': \"\n",
      "                                        \"str(sys.argv[1]),\\\\n    'project_id': \"\n",
      "                                        \"str(sys.argv[2]),\\\\n    'dataset_id': \"\n",
      "                                        'str(sys.argv[3]),\\\\n    '\n",
      "                                        \"'order_summaries_table_id': \"\n",
      "                                        'str(sys.argv[4]),\\\\n    '\n",
      "                                        \"'threshold_date': \"\n",
      "                                        'str(sys.argv[5]),\\\\n    '\n",
      "                                        \"'max_monetary': \"\n",
      "                                        'int(sys.argv[6]),\\\\n}\\\\n_output_files '\n",
      "                                        '= [\\\\n    '\n",
      "                                        'sys.argv[7],\\\\n]\\\\n\\\\n_outputs = '\n",
      "                                        'generate_feature_engineering_query(**_args)\\\\n\\\\nif '\n",
      "                                        \"not hasattr(_outputs, '__getitem__') \"\n",
      "                                        'or isinstance(_outputs, str):\\\\n    '\n",
      "                                        '_outputs = [_outputs]\\\\n\\\\nfrom '\n",
      "                                        'pathlib import Path\\\\nfor idx, '\n",
      "                                        'filename in '\n",
      "                                        'enumerate(_output_files):\\\\n    '\n",
      "                                        '_output_path = Path(filename)\\\\n    '\n",
      "                                        '_output_path.parent.mkdir(parents=True, '\n",
      "                                        'exist_ok=True)\\\\n    '\n",
      "                                        '_output_path.write_text(str(_outputs[idx]))\\\\n\"], '\n",
      "                                        '\"args\": [\"\\\\nSELECT\\\\n  '\n",
      "                                        'tf.customer_id,\\\\n  -- For training '\n",
      "                                        'period\\\\n  -- Copying the '\n",
      "                                        'calculations from Lifetimes where '\n",
      "                                        'first orders are ignored\\\\n  -- See '\n",
      "                                        'https://github.com/CamDavidsonPilon/lifetimes/blob/master/lifetimes/utils.py#L246\\\\n--[START '\n",
      "                                        'features_target]\\\\n  '\n",
      "                                        'ROUND(tf.monetary, 2) as '\n",
      "                                        'monetary,\\\\n  tf.cnt_orders AS '\n",
      "                                        'frequency,\\\\n  tf.recency,\\\\n  '\n",
      "                                        'tf.T,\\\\n  '\n",
      "                                        'ROUND(tf.recency/cnt_orders, 2) AS '\n",
      "                                        'time_between,\\\\n  '\n",
      "                                        'ROUND(tf.avg_basket_value, 2) AS '\n",
      "                                        'avg_basket_value,\\\\n  '\n",
      "                                        'ROUND(tf.avg_basket_size, 2) AS '\n",
      "                                        'avg_basket_size,\\\\n  '\n",
      "                                        'tf.cnt_returns,\\\\n  -- Target '\n",
      "                                        'calculated for overall period\\\\n  '\n",
      "                                        'ROUND(tt.target_monetary, 2) as '\n",
      "                                        'target_monetary\\\\n--[END '\n",
      "                                        'features_target]\\\\nFROM\\\\n  -- This '\n",
      "                                        'SELECT uses only data before '\n",
      "                                        'threshold to make features.\\\\n  '\n",
      "                                        '(\\\\n    SELECT\\\\n      '\n",
      "                                        'customer_id,\\\\n      SUM(order_value) '\n",
      "                                        'AS monetary,\\\\n      '\n",
      "                                        'DATE_DIFF(MAX(order_date), '\n",
      "                                        'MIN(order_date), DAY) AS '\n",
      "                                        'recency,\\\\n      '\n",
      "                                        \"DATE_DIFF(DATE('<<threshold_date>>'), \"\n",
      "                                        'MIN(order_date), DAY) AS T,\\\\n      '\n",
      "                                        'COUNT(DISTINCT order_date) AS '\n",
      "                                        'cnt_orders,\\\\n      '\n",
      "                                        'AVG(order_qty_articles) '\n",
      "                                        'avg_basket_size,\\\\n      '\n",
      "                                        'AVG(order_value) '\n",
      "                                        'avg_basket_value,\\\\n      '\n",
      "                                        'SUM(CASE\\\\n          WHEN order_value '\n",
      "                                        '< 1 THEN 1\\\\n          ELSE 0 END) AS '\n",
      "                                        'cnt_returns\\\\n    FROM\\\\n      -- '\n",
      "                                        'Makes the order value = 0 if it is '\n",
      "                                        'the first one\\\\n      (\\\\n        '\n",
      "                                        'SELECT\\\\n          a.*,\\\\n          '\n",
      "                                        '(CASE\\\\n              WHEN '\n",
      "                                        'a.order_date = c.order_date_min THEN '\n",
      "                                        '0\\\\n              ELSE a.order_value '\n",
      "                                        'END) AS order_value_btyd\\\\n        '\n",
      "                                        'FROM\\\\n          '\n",
      "                                        '`<<project_id>>.<<dataset_id>>.<<order_summaries_table_id>>` '\n",
      "                                        'a\\\\n        INNER JOIN (\\\\n          '\n",
      "                                        'SELECT\\\\n            '\n",
      "                                        'customer_id,\\\\n            '\n",
      "                                        'MIN(order_date) AS '\n",
      "                                        'order_date_min\\\\n          '\n",
      "                                        'FROM\\\\n            '\n",
      "                                        '`<<project_id>>.<<dataset_id>>.<<order_summaries_table_id>>`\\\\n          '\n",
      "                                        'GROUP BY\\\\n            customer_id) '\n",
      "                                        'c\\\\n        ON\\\\n          '\n",
      "                                        'c.customer_id = a.customer_id\\\\n      '\n",
      "                                        ')\\\\n    WHERE\\\\n      order_date <= '\n",
      "                                        \"DATE('<<threshold_date>>')\\\\n    \"\n",
      "                                        'GROUP BY\\\\n      customer_id) '\n",
      "                                        'tf,\\\\n\\\\n  -- This SELECT uses all '\n",
      "                                        'records to calculate the target '\n",
      "                                        '(could also use data after threshold '\n",
      "                                        ')\\\\n  (\\\\n    SELECT\\\\n      '\n",
      "                                        'customer_id,\\\\n      SUM(order_value) '\n",
      "                                        'target_monetary\\\\n    FROM\\\\n      '\n",
      "                                        '`<<project_id>>.<<dataset_id>>.<<order_summaries_table_id>>`\\\\n      '\n",
      "                                        '--WHERE order_date > '\n",
      "                                        \"DATE('<<threshold_date>>')\\\\n    \"\n",
      "                                        'GROUP BY\\\\n      customer_id) '\n",
      "                                        'tt\\\\nWHERE\\\\n  tf.customer_id = '\n",
      "                                        'tt.customer_id\\\\n  AND tf.monetary > '\n",
      "                                        '0\\\\n  AND tf.monetary <= '\n",
      "                                        '<<max_monetary>>\\\\n\\\\n\", '\n",
      "                                        '\"{{inputs.parameters.project-id}}\", '\n",
      "                                        '\"{{inputs.parameters.dataset-id}}\", '\n",
      "                                        '\"order_summaries\", '\n",
      "                                        '\"{{inputs.parameters.threshold-date}}\", '\n",
      "                                        '\"{{inputs.parameters.max-monetary}}\", '\n",
      "                                        '\"/outputs/Output/data\"]}}, {\"name\": '\n",
      "                                        '\"import-from-bq\", \"outputs\": '\n",
      "                                        '{\"parameters\": [{\"name\": '\n",
      "                                        '\"import-dataset-automl-dataset-id\", '\n",
      "                                        '\"valueFrom\": {\"path\": '\n",
      "                                        '\"/outputs/automl_dataset_id/data\"}}, '\n",
      "                                        '{\"name\": '\n",
      "                                        '\"import-dataset-automl-dataset-name\", '\n",
      "                                        '\"valueFrom\": {\"path\": '\n",
      "                                        '\"/outputs/automl_dataset_name/data\"}}], '\n",
      "                                        '\"artifacts\": [{\"name\": '\n",
      "                                        '\"mlpipeline-ui-metadata\", \"path\": '\n",
      "                                        '\"/mlpipeline-ui-metadata.json\", \"s3\": '\n",
      "                                        '{\"insecure\": true, \"accessKeySecret\": '\n",
      "                                        '{\"name\": \"mlpipeline-minio-artifact\", '\n",
      "                                        '\"key\": \"accesskey\"}, \"endpoint\": '\n",
      "                                        '\"minio-service.kubeflow:9000\", '\n",
      "                                        '\"secretKeySecret\": {\"name\": '\n",
      "                                        '\"mlpipeline-minio-artifact\", \"key\": '\n",
      "                                        '\"secretkey\"}, \"key\": '\n",
      "                                        '\"runs/{{workflow.uid}}/{{pod.name}}/mlpipeline-ui-metadata.tgz\", '\n",
      "                                        '\"bucket\": \"mlpipeline\"}}, {\"name\": '\n",
      "                                        '\"mlpipeline-metrics\", \"path\": '\n",
      "                                        '\"/mlpipeline-metrics.json\", \"s3\": '\n",
      "                                        '{\"insecure\": true, \"accessKeySecret\": '\n",
      "                                        '{\"name\": \"mlpipeline-minio-artifact\", '\n",
      "                                        '\"key\": \"accesskey\"}, \"endpoint\": '\n",
      "                                        '\"minio-service.kubeflow:9000\", '\n",
      "                                        '\"secretKeySecret\": {\"name\": '\n",
      "                                        '\"mlpipeline-minio-artifact\", \"key\": '\n",
      "                                        '\"secretkey\"}, \"key\": '\n",
      "                                        '\"runs/{{workflow.uid}}/{{pod.name}}/mlpipeline-metrics.tgz\", '\n",
      "                                        '\"bucket\": \"mlpipeline\"}}]}, \"inputs\": '\n",
      "                                        '{\"parameters\": [{\"name\": '\n",
      "                                        '\"automl-dataset-location\"}, {\"name\": '\n",
      "                                        '\"dataset-id\"}, {\"name\": '\n",
      "                                        '\"project-id\"}]}, \"container\": '\n",
      "                                        '{\"image\": '\n",
      "                                        '\"gcr.io/sandbox-235500/automltablesbase:dev\", '\n",
      "                                        '\"command\": [\"python3\", \"-c\", \"from '\n",
      "                                        'typing import NamedTuple\\\\n\\\\ndef '\n",
      "                                        'import_dataset(\\\\n    location: '\n",
      "                                        'str,\\\\n    project_id: str,\\\\n    '\n",
      "                                        'dataset_id: str,\\\\n    table_id: '\n",
      "                                        'str,\\\\n    display_name: str) -> '\n",
      "                                        \"NamedTuple('DatasetInfo', \"\n",
      "                                        '\\\\n                               '\n",
      "                                        \"[('automl_dataset_name', str), \"\n",
      "                                        '\\\\n                                '\n",
      "                                        \"('automl_dataset_id', str)]):\\\\n    \"\n",
      "                                        '\\\\\"\\\\\"\\\\\"Creates an AutoML Tables '\n",
      "                                        'dataset from the data in '\n",
      "                                        'BigQuery.\\\\\"\\\\\"\\\\\"\\\\n    \\\\n    from '\n",
      "                                        'collections import namedtuple\\\\n    '\n",
      "                                        'from google.cloud import '\n",
      "                                        'automl_v1beta1 as automl\\\\n    \\\\n    '\n",
      "                                        'client = automl.AutoMlClient()\\\\n    '\n",
      "                                        '# Create dataset\\\\n    location_path '\n",
      "                                        '= client.location_path(project_id, '\n",
      "                                        'location)\\\\n    '\n",
      "                                        'create_dataset_response = '\n",
      "                                        'client.create_dataset(\\\\n        '\n",
      "                                        'location_path,\\\\n        '\n",
      "                                        \"{\\\\n            'display_name': \"\n",
      "                                        'display_name,\\\\n            '\n",
      "                                        \"'tables_dataset_metadata': \"\n",
      "                                        '{}\\\\n        })\\\\n    \\\\n    path = '\n",
      "                                        '\\\\\"bq://{}.{}.{}\\\\\".format(project_id, '\n",
      "                                        'dataset_id, table_id)\\\\n    '\n",
      "                                        'input_config = '\n",
      "                                        '{\\\\\"bigquery_source\\\\\": '\n",
      "                                        '{\\\\\"input_uri\\\\\": path}}\\\\n    '\n",
      "                                        'import_data_response = '\n",
      "                                        'client.import_data(create_dataset_response.name, '\n",
      "                                        'input_config)\\\\n    '\n",
      "                                        'print(\\\\\"Initiating import '\n",
      "                                        '...\\\\\")\\\\n    # synchronous check of '\n",
      "                                        'operation status.\\\\n    print(\\\\\"Data '\n",
      "                                        'imported. '\n",
      "                                        '{}\\\\\".format(import_data_response.result()))\\\\n    '\n",
      "                                        '\\\\n    # Return component '\n",
      "                                        'outputs\\\\n    result = '\n",
      "                                        \"namedtuple('DatasetInfo', \"\n",
      "                                        \"['automl_dataset_name', \"\n",
      "                                        \"'automl_dataset_id'])\\\\n    return \"\n",
      "                                        'result(create_dataset_response.display_name, '\n",
      "                                        'create_dataset_response.name)\\\\n\\\\nimport '\n",
      "                                        \"sys\\\\n_args = {\\\\n    'location': \"\n",
      "                                        \"str(sys.argv[1]),\\\\n    'project_id': \"\n",
      "                                        \"str(sys.argv[2]),\\\\n    'dataset_id': \"\n",
      "                                        \"str(sys.argv[3]),\\\\n    'table_id': \"\n",
      "                                        'str(sys.argv[4]),\\\\n    '\n",
      "                                        \"'display_name': \"\n",
      "                                        'str(sys.argv[5]),\\\\n}\\\\n_output_files '\n",
      "                                        '= [\\\\n    sys.argv[6],\\\\n    '\n",
      "                                        'sys.argv[7],\\\\n]\\\\n\\\\n_outputs = '\n",
      "                                        'import_dataset(**_args)\\\\n\\\\nif not '\n",
      "                                        \"hasattr(_outputs, '__getitem__') or \"\n",
      "                                        'isinstance(_outputs, str):\\\\n    '\n",
      "                                        '_outputs = [_outputs]\\\\n\\\\nfrom '\n",
      "                                        'pathlib import Path\\\\nfor idx, '\n",
      "                                        'filename in '\n",
      "                                        'enumerate(_output_files):\\\\n    '\n",
      "                                        '_output_path = Path(filename)\\\\n    '\n",
      "                                        '_output_path.parent.mkdir(parents=True, '\n",
      "                                        'exist_ok=True)\\\\n    '\n",
      "                                        '_output_path.write_text(str(_outputs[idx]))\\\\n\"], '\n",
      "                                        '\"args\": '\n",
      "                                        '[\"{{inputs.parameters.automl-dataset-location}}\", '\n",
      "                                        '\"{{inputs.parameters.project-id}}\", '\n",
      "                                        '\"{{inputs.parameters.dataset-id}}\", '\n",
      "                                        '\"clv_features\", \"CLVFeatures\", '\n",
      "                                        '\"/outputs/automl_dataset_name/data\", '\n",
      "                                        '\"/outputs/automl_dataset_id/data\"]}}, '\n",
      "                                        '{\"name\": \"preprocessing-query\", '\n",
      "                                        '\"outputs\": {\"parameters\": [{\"name\": '\n",
      "                                        '\"preprocessing-query-output\", '\n",
      "                                        '\"valueFrom\": {\"path\": '\n",
      "                                        '\"/outputs/Output/data\"}}], '\n",
      "                                        '\"artifacts\": [{\"name\": '\n",
      "                                        '\"mlpipeline-ui-metadata\", \"path\": '\n",
      "                                        '\"/mlpipeline-ui-metadata.json\", \"s3\": '\n",
      "                                        '{\"insecure\": true, \"accessKeySecret\": '\n",
      "                                        '{\"name\": \"mlpipeline-minio-artifact\", '\n",
      "                                        '\"key\": \"accesskey\"}, \"endpoint\": '\n",
      "                                        '\"minio-service.kubeflow:9000\", '\n",
      "                                        '\"secretKeySecret\": {\"name\": '\n",
      "                                        '\"mlpipeline-minio-artifact\", \"key\": '\n",
      "                                        '\"secretkey\"}, \"key\": '\n",
      "                                        '\"runs/{{workflow.uid}}/{{pod.name}}/mlpipeline-ui-metadata.tgz\", '\n",
      "                                        '\"bucket\": \"mlpipeline\"}}, {\"name\": '\n",
      "                                        '\"mlpipeline-metrics\", \"path\": '\n",
      "                                        '\"/mlpipeline-metrics.json\", \"s3\": '\n",
      "                                        '{\"insecure\": true, \"accessKeySecret\": '\n",
      "                                        '{\"name\": \"mlpipeline-minio-artifact\", '\n",
      "                                        '\"key\": \"accesskey\"}, \"endpoint\": '\n",
      "                                        '\"minio-service.kubeflow:9000\", '\n",
      "                                        '\"secretKeySecret\": {\"name\": '\n",
      "                                        '\"mlpipeline-minio-artifact\", \"key\": '\n",
      "                                        '\"secretkey\"}, \"key\": '\n",
      "                                        '\"runs/{{workflow.uid}}/{{pod.name}}/mlpipeline-metrics.tgz\", '\n",
      "                                        '\"bucket\": \"mlpipeline\"}}]}, \"inputs\": '\n",
      "                                        '{\"parameters\": [{\"name\": '\n",
      "                                        '\"dataset-id\"}, {\"name\": '\n",
      "                                        '\"predict-end\"}, {\"name\": '\n",
      "                                        '\"project-id\"}, {\"name\": '\n",
      "                                        '\"threshold-date\"}, {\"name\": '\n",
      "                                        '\"transactions-table-id\"}]}, '\n",
      "                                        '\"container\": {\"image\": '\n",
      "                                        '\"gcr.io/sandbox-235500/automltablesbase:dev\", '\n",
      "                                        '\"command\": [\"python3\", \"-c\", \"def '\n",
      "                                        'generate_data_preprocessing_query(\\\\n    '\n",
      "                                        'query_template: str,\\\\n    '\n",
      "                                        'project_id: str,\\\\n    dataset_id: '\n",
      "                                        'str,\\\\n    transactions_table_id: '\n",
      "                                        'str,\\\\n    threshold_date: str,\\\\n    '\n",
      "                                        'predict_end: str) -> str:\\\\n    '\n",
      "                                        '\\\\\"\\\\\"\\\\\"Parametrizes clean data '\n",
      "                                        'query\\\\\"\\\\\"\\\\\"\\\\n    \\\\n    query = '\n",
      "                                        'str(query_template)\\\\n    query = '\n",
      "                                        'query.replace(\\\\\"<<project_id>>\\\\\", '\n",
      "                                        'str(project_id))\\\\n    query = '\n",
      "                                        'query.replace(\\\\\"<<dataset_id>>\\\\\", '\n",
      "                                        'str(dataset_id))\\\\n    query = '\n",
      "                                        'query.replace(\\\\\"<<threshold_date>>\\\\\", '\n",
      "                                        'str(threshold_date))\\\\n    query = '\n",
      "                                        'query.replace(\\\\\"<<predict_end>>\\\\\", '\n",
      "                                        'str(predict_end))\\\\n    query = '\n",
      "                                        'query.replace(\\\\\"<<transactions_table_id>>\\\\\", '\n",
      "                                        'str(transactions_table_id))\\\\n    '\n",
      "                                        '\\\\n    return query\\\\n\\\\nimport '\n",
      "                                        'sys\\\\n_args = {\\\\n    '\n",
      "                                        \"'query_template': \"\n",
      "                                        \"str(sys.argv[1]),\\\\n    'project_id': \"\n",
      "                                        \"str(sys.argv[2]),\\\\n    'dataset_id': \"\n",
      "                                        'str(sys.argv[3]),\\\\n    '\n",
      "                                        \"'transactions_table_id': \"\n",
      "                                        'str(sys.argv[4]),\\\\n    '\n",
      "                                        \"'threshold_date': \"\n",
      "                                        'str(sys.argv[5]),\\\\n    '\n",
      "                                        \"'predict_end': \"\n",
      "                                        'str(sys.argv[6]),\\\\n}\\\\n_output_files '\n",
      "                                        '= [\\\\n    '\n",
      "                                        'sys.argv[7],\\\\n]\\\\n\\\\n_outputs = '\n",
      "                                        'generate_data_preprocessing_query(**_args)\\\\n\\\\nif '\n",
      "                                        \"not hasattr(_outputs, '__getitem__') \"\n",
      "                                        'or isinstance(_outputs, str):\\\\n    '\n",
      "                                        '_outputs = [_outputs]\\\\n\\\\nfrom '\n",
      "                                        'pathlib import Path\\\\nfor idx, '\n",
      "                                        'filename in '\n",
      "                                        'enumerate(_output_files):\\\\n    '\n",
      "                                        '_output_path = Path(filename)\\\\n    '\n",
      "                                        '_output_path.parent.mkdir(parents=True, '\n",
      "                                        'exist_ok=True)\\\\n    '\n",
      "                                        '_output_path.write_text(str(_outputs[idx]))\\\\n\"], '\n",
      "                                        '\"args\": [\"\\\\nSELECT\\\\n  '\n",
      "                                        'a.customer_id,\\\\n  a.order_date,\\\\n  '\n",
      "                                        'a.order_value,\\\\n  '\n",
      "                                        'a.order_qty_articles\\\\nFROM\\\\n(\\\\n  '\n",
      "                                        'SELECT\\\\n    customer_id,\\\\n    '\n",
      "                                        'order_date,\\\\n    '\n",
      "                                        'ROUND(SUM(unit_price * quantity), 2) '\n",
      "                                        'AS order_value,\\\\n    SUM(quantity) '\n",
      "                                        'AS order_qty_articles,\\\\n    '\n",
      "                                        '(\\\\n      SELECT\\\\n        '\n",
      "                                        'MAX(order_date)\\\\n      '\n",
      "                                        'FROM\\\\n        '\n",
      "                                        '`<<project_id>>.<<dataset_id>>.<<transactions_table_id>>` '\n",
      "                                        'tl\\\\n      WHERE\\\\n        '\n",
      "                                        'tl.customer_id = t.customer_id\\\\n    '\n",
      "                                        ') latest_order\\\\n  FROM\\\\n    '\n",
      "                                        '`<<project_id>>.<<dataset_id>>.<<transactions_table_id>>` '\n",
      "                                        't\\\\n  GROUP BY\\\\n      '\n",
      "                                        'customer_id,\\\\n      order_date\\\\n) '\n",
      "                                        'a\\\\n\\\\nINNER JOIN (\\\\n  -- Only '\n",
      "                                        'customers with more than one positive '\n",
      "                                        'order values before threshold.\\\\n  '\n",
      "                                        'SELECT\\\\n    customer_id\\\\n  FROM '\n",
      "                                        '(\\\\n    -- Customers and how many '\n",
      "                                        'positive order values  before '\n",
      "                                        'threshold.\\\\n    SELECT\\\\n      '\n",
      "                                        'customer_id,\\\\n      '\n",
      "                                        'SUM(positive_value) '\n",
      "                                        'cnt_positive_value\\\\n    FROM '\n",
      "                                        '(\\\\n      -- Customer with whether '\n",
      "                                        'order was positive or not at each '\n",
      "                                        'date.\\\\n      SELECT\\\\n        '\n",
      "                                        'customer_id,\\\\n        (\\\\n          '\n",
      "                                        'CASE\\\\n            WHEN '\n",
      "                                        'SUM(unit_price * quantity) > 0 THEN '\n",
      "                                        '1\\\\n            ELSE 0\\\\n          '\n",
      "                                        'END ) positive_value\\\\n      '\n",
      "                                        'FROM\\\\n        '\n",
      "                                        '`<<project_id>>.<<dataset_id>>.<<transactions_table_id>>`\\\\n      '\n",
      "                                        'WHERE\\\\n        order_date < '\n",
      "                                        'DATE(\\\\\"<<threshold_date>>\\\\\")\\\\n      '\n",
      "                                        'GROUP BY\\\\n        '\n",
      "                                        'customer_id,\\\\n        '\n",
      "                                        'order_date)\\\\n    GROUP BY\\\\n      '\n",
      "                                        'customer_id )\\\\n  WHERE\\\\n    '\n",
      "                                        'cnt_positive_value > 1\\\\n  ) '\n",
      "                                        'b\\\\nON\\\\n  a.customer_id = '\n",
      "                                        'b.customer_id\\\\n--[START '\n",
      "                                        'common_clean]\\\\nWHERE\\\\n  -- Bought '\n",
      "                                        'in the past 3 months\\\\n  '\n",
      "                                        'DATE_DIFF(DATE(\\\\\"<<predict_end>>\\\\\"), '\n",
      "                                        'latest_order, DAY) <= 90\\\\n  -- Make '\n",
      "                                        'sure returns are consistent.\\\\n  AND '\n",
      "                                        '(\\\\n    (order_qty_articles > 0 and '\n",
      "                                        'order_Value > 0) OR\\\\n    '\n",
      "                                        '(order_qty_articles < 0 and '\n",
      "                                        'order_Value < 0)\\\\n  )\\\\n\", '\n",
      "                                        '\"{{inputs.parameters.project-id}}\", '\n",
      "                                        '\"{{inputs.parameters.dataset-id}}\", '\n",
      "                                        '\"{{inputs.parameters.transactions-table-id}}\", '\n",
      "                                        '\"{{inputs.parameters.threshold-date}}\", '\n",
      "                                        '\"{{inputs.parameters.predict-end}}\", '\n",
      "                                        '\"/outputs/Output/data\"]}}], '\n",
      "                                        '\"entrypoint\": \"clvpipeline\", '\n",
      "                                        '\"arguments\": {\"parameters\": [{\"name\": '\n",
      "                                        '\"project-id\", \"value\": \"\"}, {\"name\": '\n",
      "                                        '\"dataset-id\", \"value\": \"\"}, {\"name\": '\n",
      "                                        '\"transactions-table-id\", \"value\": '\n",
      "                                        '\"\"}, {\"name\": \"threshold-date\", '\n",
      "                                        '\"value\": \"\"}, {\"name\": \"predict-end\", '\n",
      "                                        '\"value\": \"\"}, {\"name\": \"model-name\", '\n",
      "                                        '\"value\": \"\"}, {\"name\": '\n",
      "                                        '\"max-monetary\", \"value\": \"15000\"}, '\n",
      "                                        '{\"name\": \"bq-dataset-location\", '\n",
      "                                        '\"value\": \"US\"}, {\"name\": '\n",
      "                                        '\"automl-dataset-location\", \"value\": '\n",
      "                                        '\"us-central1\"}]}}, \"kind\": '\n",
      "                                        '\"Workflow\", \"metadata\": '\n",
      "                                        '{\"generateName\": \"clvpipeline-\"}, '\n",
      "                                        '\"apiVersion\": '\n",
      "                                        '\"argoproj.io/v1alpha1\"}'},\n",
      " 'resource_references': [{'key': {'id': '467df5ba-949c-4781-8478-48fb3ae77859',\n",
      "                                  'type': 'EXPERIMENT'},\n",
      "                          'relationship': 'OWNER'}],\n",
      " 'scheduled_at': datetime.datetime(1970, 1, 1, 0, 0, tzinfo=tzlocal()),\n",
      " 'status': None,\n",
      " 'storage_state': None}\n"
     ]
    }
   ],
   "source": [
    "#Specify pipeline argument values\n",
    "arguments = {\n",
    "    'project_id': 'sandbox-235500',\n",
    "    'dataset_id': 'CLVDataset',\n",
    "    'transactions_table_id': 'transactions',\n",
    "    'threshold_date': '2011-08-08',\n",
    "    'predict_end': '2011-12-12',\n",
    "    'max_monetary': '15000',\n",
    "    'model_name': 'CLVModel'\n",
    "}\n",
    "\n",
    "\n",
    "HOST = 'http://localhost:8082/api/v1/namespaces/kubeflow/services/ml-pipeline:8888/proxy'\n",
    "EXPERIMENT_NAME = 'CLV_TRAINING'\n",
    "\n",
    "client = kfp.Client(HOST)\n",
    "experiment = client.create_experiment(EXPERIMENT_NAME)\n",
    "\n",
    "#Submit a pipeline run\n",
    "run_name = pipeline_func.__name__ + ' run'\n",
    "run_result = client.run_pipeline(experiment.id, run_name, pipeline_filename, arguments)\n",
    "print(run_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "KFP",
   "language": "python",
   "name": "kfp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
