{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "#!{sys.executable} -m pip install google-cloud-bigquery[pandas]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The google.cloud.bigquery extension is already loaded. To reload it, use:\n",
      "  %reload_ext google.cloud.bigquery\n"
     ]
    }
   ],
   "source": [
    "%load_ext google.cloud.bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nSELECT\\n  customer_id,\\n  order_date,\\n  order_value,\\n  order_qty_articles\\nFROM\\n(\\n  SELECT\\n    CustomerID AS customer_id,\\n    PARSE_DATE(\"%m/%d/%y\", SUBSTR(InvoiceDate, 0, 8)) AS order_date,\\n    ROUND(SUM(UnitPrice * Quantity), 2) AS order_value,\\n    SUM(Quantity) AS order_qty_articles,\\n    (\\n      SELECT\\n        MAX(PARSE_DATE(\"%m/%d/%y\", SUBSTR(InvoiceDate, 0, 8)))\\n      FROM\\n        `sandbox-235500.CLVDataset.data_source` tl\\n      WHERE\\n        tl.CustomerID = t.CustomerID\\n    ) latest_order\\n  FROM\\n    `sandbox-235500.CLVDataset.data_source` t\\n  GROUP BY\\n      CustomerID,\\n      order_date\\n) a\\n\\nINNER JOIN (\\n  -- Only customers with more than one positive order values before threshold.\\n  SELECT\\n    CustomerID\\n  FROM (\\n    -- Customers and how many positive order values  before threshold.\\n    SELECT\\n      CustomerID,\\n      SUM(positive_value) cnt_positive_value\\n    FROM (\\n      -- Customer with whether order was positive or not at each date.\\n      SELECT\\n        CustomerID,\\n        (\\n          CASE\\n            WHEN SUM(UnitPrice * Quantity) > 0 THEN 1\\n            ELSE 0\\n          END ) positive_value\\n      FROM\\n        `sandbox-235500.CLVDataset.data_source`\\n      WHERE\\n        PARSE_DATE(\"%m/%d/%y\", SUBSTR(InvoiceDate, 0, 8)) < DATE(\"2011-3-1\")\\n      GROUP BY\\n        CustomerID,\\n        SUBSTR(InvoiceDate, 0, 8) )\\n    GROUP BY\\n      CustomerID )\\n  WHERE\\n    cnt_positive_value > 1\\n  ) b\\nON\\n  a.customer_id = b. CustomerID\\n--[START common_clean]\\nWHERE\\n  -- Bought in the past 3 months\\n  DATE_DIFF(DATE(\"2011-3-1\"), latest_order, DAY) <= 90\\n  -- Make sure returns are consistent.\\n  AND (\\n    (order_qty_articles > 0 and order_Value > 0) OR\\n    (order_qty_articles < 0 and order_Value < 0)\\n  )\\n'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threshold_date = '2011-3-1'\n",
    "predict_date = '2011-3-1'\n",
    "\n",
    "QUERY = '''\n",
    "SELECT\n",
    "  customer_id,\n",
    "  order_date,\n",
    "  order_value,\n",
    "  order_qty_articles\n",
    "FROM\n",
    "(\n",
    "  SELECT\n",
    "    CustomerID AS customer_id,\n",
    "    PARSE_DATE(\"%m/%d/%y\", SUBSTR(InvoiceDate, 0, 8)) AS order_date,\n",
    "    ROUND(SUM(UnitPrice * Quantity), 2) AS order_value,\n",
    "    SUM(Quantity) AS order_qty_articles,\n",
    "    (\n",
    "      SELECT\n",
    "        MAX(PARSE_DATE(\"%m/%d/%y\", SUBSTR(InvoiceDate, 0, 8)))\n",
    "      FROM\n",
    "        `<<project_id>>.<<dataset_id>>.data_source` tl\n",
    "      WHERE\n",
    "        tl.CustomerID = t.CustomerID\n",
    "    ) latest_order\n",
    "  FROM\n",
    "    `<<project_id>>.<<dataset_id>>.data_source` t\n",
    "  GROUP BY\n",
    "      CustomerID,\n",
    "      order_date\n",
    ") a\n",
    "\n",
    "INNER JOIN (\n",
    "  -- Only customers with more than one positive order values before threshold.\n",
    "  SELECT\n",
    "    CustomerID\n",
    "  FROM (\n",
    "    -- Customers and how many positive order values  before threshold.\n",
    "    SELECT\n",
    "      CustomerID,\n",
    "      SUM(positive_value) cnt_positive_value\n",
    "    FROM (\n",
    "      -- Customer with whether order was positive or not at each date.\n",
    "      SELECT\n",
    "        CustomerID,\n",
    "        (\n",
    "          CASE\n",
    "            WHEN SUM(UnitPrice * Quantity) > 0 THEN 1\n",
    "            ELSE 0\n",
    "          END ) positive_value\n",
    "      FROM\n",
    "        `<<project_id>>.<<dataset_id>>.data_source`\n",
    "      WHERE\n",
    "        PARSE_DATE(\"%m/%d/%y\", SUBSTR(InvoiceDate, 0, 8)) < DATE(\"<<threshold_date>>\")\n",
    "      GROUP BY\n",
    "        CustomerID,\n",
    "        SUBSTR(InvoiceDate, 0, 8) )\n",
    "    GROUP BY\n",
    "      CustomerID )\n",
    "  WHERE\n",
    "    cnt_positive_value > 1\n",
    "  ) b\n",
    "ON\n",
    "  a.customer_id = b. CustomerID\n",
    "--[START common_clean]\n",
    "WHERE\n",
    "  -- Bought in the past 3 months\n",
    "  DATE_DIFF(DATE(\"<<predict_date>>\"), latest_order, DAY) <= 90\n",
    "  -- Make sure returns are consistent.\n",
    "  AND (\n",
    "    (order_qty_articles > 0 and order_Value > 0) OR\n",
    "    (order_qty_articles < 0 and order_Value < 0)\n",
    "  )\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "threshold_date = '2011-3-1'\n",
    "predict_date = '2011-3-1'\n",
    "project_id = 'sandbox-235500'\n",
    "dataset_id = \"CLVDataset\"\n",
    "\n",
    "QUERY = QUERY.replace(\"<<threshold_date>>\", threshold_date)\n",
    "QUERY = QUERY.replace(\"<<predict_date>>\", predict_date)\n",
    "QUERY = QUERY.replace(\"<<project_id>>\", project_id)\n",
    "QUERY = QUERY.replace(\"<<dataset_id>>\", dataset_id)\n",
    "QUERY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFound",
     "evalue": "404 Not found: Table sandbox-235500:CLVDataset.data_cleaned was not found in location US",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFound\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-cc9aa2caec51>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mquery_job\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQUERY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'US'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjob_config\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# API request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Waits for query to finish\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/kfp/lib/python3.5/site-packages/google/cloud/bigquery/job.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout, retry)\u001b[0m\n\u001b[1;32m   2792\u001b[0m             \u001b[0;32mnot\u001b[0m \u001b[0mcomplete\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mgiven\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2793\u001b[0m         \"\"\"\n\u001b[0;32m-> 2794\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQueryJob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2795\u001b[0m         \u001b[0;31m# Return an iterator instead of returning the job.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2796\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_query_results\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/kfp/lib/python3.5/site-packages/google/cloud/bigquery/job.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout, retry)\u001b[0m\n\u001b[1;32m    705\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m         \u001b[0;31m# TODO: modify PollingFuture so it can pass a retry argument to done().\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_AsyncJob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    708\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcancelled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/kfp/lib/python3.5/site-packages/google/api_core/future/polling.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0;31m# pylint: disable=raising-bad-type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0;31m# Pylint doesn't recognize that this is valid in this case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFound\u001b[0m: 404 Not found: Table sandbox-235500:CLVDataset.data_cleaned was not found in location US"
     ]
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "\n",
    "\n",
    "job_config = bigquery.QueryJobConfig()\n",
    "table_ref = client.dataset(dataset_id).table('data_cleaned')\n",
    "job_config.destination = table_ref\n",
    "job_config.create_disposition = bigquery.job.CreateDisposition.CREATE_IF_NEEDED\n",
    "job_config.write_disposition = bigquery.job.WriteDisposition.WRITE_TRUNCATE\n",
    "\n",
    "client = bigquery.Client()\n",
    "\n",
    "query_job = client.query(QUERY, location='US', job_config=job_config)  # API request\n",
    "rows = iter(query_job.result())  # Waits for query to finish\n",
    "\n",
    "for _ in range(5):\n",
    "    print(next(rows))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nSELECT\\n  tf.customer_id,\\n  -- For training period\\n  -- Copying the calculations from Lifetimes where first orders are ignored\\n  -- See https://github.com/CamDavidsonPilon/lifetimes/blob/master/lifetimes/utils.py#L246\\n--[START features_target]\\n  tf.monetary_dnn,\\n  tf.monetary_btyd,\\n  tf.cnt_orders AS frequency_dnn,\\n  tf.cnt_orders - 1 AS frequency_btyd,\\n  tf.recency,\\n  tf.T,\\n  ROUND(tf.recency/cnt_orders, 2) AS time_between,\\n  ROUND(tf.avg_basket_value, 2) AS avg_basket_value,\\n  ROUND(tf.avg_basket_size, 2) AS avg_basket_size,\\n  tf.cnt_returns,\\n  (CASE\\n      WHEN tf.cnt_returns > 0 THEN 1\\n      ELSE 0 END) AS has_returned,\\n\\n  -- Used by BTYD mainly, potentially DNN if clipped improve results\\n  (CASE\\n      WHEN tf.cnt_orders - 1 > 600 THEN 600\\n      ELSE tf.cnt_orders - 1 END) AS frequency_btyd_clipped,\\n  (CASE\\n      WHEN tf.monetary_btyd > 100000 THEN 100000\\n      ELSE ROUND(tf.monetary_btyd, 2) END) AS monetary_btyd_clipped,\\n  (CASE\\n      WHEN tt.target_monetary > 100000 THEN 100000\\n      ELSE ROUND(tt.target_monetary, 2) END) AS target_monetary_clipped,\\n\\n  -- Target calculated for overall period\\n  ROUND(tt.target_monetary, 2) as target_monetary\\n--[END features_target]\\nFROM\\n  -- This SELECT uses only data before threshold to make features.\\n  (\\n    SELECT\\n      customer_id,\\n      SUM(order_value) AS monetary_dnn,\\n      (CASE\\n        WHEN COUNT(DISTINCT order_date) = 1 THEN 0\\n        ELSE SUM(order_value_btyd) / (COUNT(DISTINCT order_date) -1) END) AS monetary_btyd,\\n      DATE_DIFF(MAX(order_date), MIN(order_date), DAY) AS recency,\\n      DATE_DIFF(DATE('2011-3-1'), MIN(order_date), DAY) AS T,\\n      COUNT(DISTINCT order_date) AS cnt_orders,\\n      AVG(order_qty_articles) avg_basket_size,\\n      AVG(order_value) avg_basket_value,\\n      SUM(CASE\\n          WHEN order_value < 1 THEN 1\\n          ELSE 0 END) AS cnt_returns\\n    FROM\\n      -- Makes the order value = 0 if it is the first one\\n      (\\n        SELECT\\n          a.*,\\n          (CASE\\n              WHEN a.order_date = c.order_date_min THEN 0\\n              ELSE a.order_value END) AS order_value_btyd\\n--[START airflow_params]\\n        FROM\\n          `sandbox-235500.CLVDataset.data_cleaned` a\\n--[END airflow_params]\\n        INNER JOIN (\\n          SELECT\\n            customer_id,\\n            MIN(order_date) AS order_date_min\\n          FROM\\n            `sandbox-235500.CLVDataset.data_cleaned`\\n          GROUP BY\\n            customer_id) c\\n        ON\\n          c.customer_id = a.customer_id\\n      )\\n    WHERE\\n--[START threshold_date]\\n      order_date <= DATE('2011-3-1')\\n--[END threshold_date]\\n    GROUP BY\\n      customer_id) tf,\\n\\n  -- This SELECT uses all records to calculate the target (could also use data after threshold )\\n  (\\n    SELECT\\n      customer_id,\\n      SUM(order_value) target_monetary\\n    FROM\\n      `sandbox-235500.CLVDataset.data_cleaned`\\n      --WHERE order_date > DATE('2011-3-1')\\n    GROUP BY\\n      customer_id) tt\\nWHERE\\n  tf.customer_id = tt.customer_id\\n  AND tf.monetary_dnn > 0\\n  AND tf.monetary_dnn <= 1000000\\n  AND tf.monetary_btyd > 0\\n\\n\""
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QUERY = '''\n",
    "SELECT\n",
    "  tf.customer_id,\n",
    "  -- For training period\n",
    "  -- Copying the calculations from Lifetimes where first orders are ignored\n",
    "  -- See https://github.com/CamDavidsonPilon/lifetimes/blob/master/lifetimes/utils.py#L246\n",
    "--[START features_target]\n",
    "  tf.monetary_dnn,\n",
    "  tf.monetary_btyd,\n",
    "  tf.cnt_orders AS frequency_dnn,\n",
    "  tf.cnt_orders - 1 AS frequency_btyd,\n",
    "  tf.recency,\n",
    "  tf.T,\n",
    "  ROUND(tf.recency/cnt_orders, 2) AS time_between,\n",
    "  ROUND(tf.avg_basket_value, 2) AS avg_basket_value,\n",
    "  ROUND(tf.avg_basket_size, 2) AS avg_basket_size,\n",
    "  tf.cnt_returns,\n",
    "  (CASE\n",
    "      WHEN tf.cnt_returns > 0 THEN 1\n",
    "      ELSE 0 END) AS has_returned,\n",
    "\n",
    "  -- Used by BTYD mainly, potentially DNN if clipped improve results\n",
    "  (CASE\n",
    "      WHEN tf.cnt_orders - 1 > 600 THEN 600\n",
    "      ELSE tf.cnt_orders - 1 END) AS frequency_btyd_clipped,\n",
    "  (CASE\n",
    "      WHEN tf.monetary_btyd > 100000 THEN 100000\n",
    "      ELSE ROUND(tf.monetary_btyd, 2) END) AS monetary_btyd_clipped,\n",
    "  (CASE\n",
    "      WHEN tt.target_monetary > 100000 THEN 100000\n",
    "      ELSE ROUND(tt.target_monetary, 2) END) AS target_monetary_clipped,\n",
    "\n",
    "  -- Target calculated for overall period\n",
    "  ROUND(tt.target_monetary, 2) as target_monetary\n",
    "--[END features_target]\n",
    "FROM\n",
    "  -- This SELECT uses only data before threshold to make features.\n",
    "  (\n",
    "    SELECT\n",
    "      customer_id,\n",
    "      SUM(order_value) AS monetary_dnn,\n",
    "      (CASE\n",
    "        WHEN COUNT(DISTINCT order_date) = 1 THEN 0\n",
    "        ELSE SUM(order_value_btyd) / (COUNT(DISTINCT order_date) -1) END) AS monetary_btyd,\n",
    "      DATE_DIFF(MAX(order_date), MIN(order_date), DAY) AS recency,\n",
    "      DATE_DIFF(DATE('<<threshold_date>>'), MIN(order_date), DAY) AS T,\n",
    "      COUNT(DISTINCT order_date) AS cnt_orders,\n",
    "      AVG(order_qty_articles) avg_basket_size,\n",
    "      AVG(order_value) avg_basket_value,\n",
    "      SUM(CASE\n",
    "          WHEN order_value < 1 THEN 1\n",
    "          ELSE 0 END) AS cnt_returns\n",
    "    FROM\n",
    "      -- Makes the order value = 0 if it is the first one\n",
    "      (\n",
    "        SELECT\n",
    "          a.*,\n",
    "          (CASE\n",
    "              WHEN a.order_date = c.order_date_min THEN 0\n",
    "              ELSE a.order_value END) AS order_value_btyd\n",
    "--[START airflow_params]\n",
    "        FROM\n",
    "          `<<project_id>>.<<dataset_id>>.data_cleaned` a\n",
    "--[END airflow_params]\n",
    "        INNER JOIN (\n",
    "          SELECT\n",
    "            customer_id,\n",
    "            MIN(order_date) AS order_date_min\n",
    "          FROM\n",
    "            `<<project_id>>.<<dataset_id>>.data_cleaned`\n",
    "          GROUP BY\n",
    "            customer_id) c\n",
    "        ON\n",
    "          c.customer_id = a.customer_id\n",
    "      )\n",
    "    WHERE\n",
    "--[START threshold_date]\n",
    "      order_date <= DATE('<<threshold_date>>')\n",
    "--[END threshold_date]\n",
    "    GROUP BY\n",
    "      customer_id) tf,\n",
    "\n",
    "  -- This SELECT uses all records to calculate the target (could also use data after threshold )\n",
    "  (\n",
    "    SELECT\n",
    "      customer_id,\n",
    "      SUM(order_value) target_monetary\n",
    "    FROM\n",
    "      `<<project_id>>.<<dataset_id>>.data_cleaned`\n",
    "      --WHERE order_date > DATE('<<threshold_date>>')\n",
    "    GROUP BY\n",
    "      customer_id) tt\n",
    "WHERE\n",
    "  tf.customer_id = tt.customer_id\n",
    "  AND tf.monetary_dnn > 0\n",
    "  AND tf.monetary_dnn <= <<max_monetary>>\n",
    "  AND tf.monetary_btyd > 0\n",
    "\n",
    "'''\n",
    "\n",
    "threshold_date = '2011-3-1'\n",
    "predict_date = '2011-3-1'\n",
    "project_id = 'sandbox-235500'\n",
    "dataset_id = \"CLVDataset\"\n",
    "max_monetary = \"1000000\"\n",
    "\n",
    "QUERY = QUERY.replace(\"<<threshold_date>>\", threshold_date)\n",
    "QUERY = QUERY.replace(\"<<predict_date>>\", predict_date)\n",
    "QUERY = QUERY.replace(\"<<project_id>>\", project_id)\n",
    "QUERY = QUERY.replace(\"<<dataset_id>>\", dataset_id)\n",
    "QUERY = QUERY.replace(\"<<max_monetary>>\", max_monetary)\n",
    "QUERY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFound",
     "evalue": "404 Not found: Table sandbox-235500:CLVDataset.data_cleaned was not found in location US",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFound\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-eca4bf1b589c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mquery_job\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQUERY\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# API request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Waits for query to finish\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/kfp/lib/python3.5/site-packages/google/cloud/bigquery/job.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout, retry)\u001b[0m\n\u001b[1;32m   2792\u001b[0m             \u001b[0;32mnot\u001b[0m \u001b[0mcomplete\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mgiven\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2793\u001b[0m         \"\"\"\n\u001b[0;32m-> 2794\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQueryJob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2795\u001b[0m         \u001b[0;31m# Return an iterator instead of returning the job.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2796\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_query_results\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/kfp/lib/python3.5/site-packages/google/cloud/bigquery/job.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout, retry)\u001b[0m\n\u001b[1;32m    705\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m         \u001b[0;31m# TODO: modify PollingFuture so it can pass a retry argument to done().\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_AsyncJob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    708\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcancelled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/kfp/lib/python3.5/site-packages/google/api_core/future/polling.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0;31m# pylint: disable=raising-bad-type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0;31m# Pylint doesn't recognize that this is valid in this case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFound\u001b[0m: 404 Not found: Table sandbox-235500:CLVDataset.data_cleaned was not found in location US"
     ]
    }
   ],
   "source": [
    "query_job = client.query(QUERY)  # API request\n",
    "rows = iter(query_job.result())  # Waits for query to finish\n",
    "\n",
    "for _ in range(5):\n",
    "    print(next(rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "KFP",
   "language": "python",
   "name": "kfp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
