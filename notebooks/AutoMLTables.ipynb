{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install KubeFlow Pipelines SDK and import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable}  -m pip install https://storage.googleapis.com/ml-pipeline/release/0.1.17/kfp.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.5.3 (default, Sep 27 2018, 17:25:39) \n",
      "[GCC 6.3.0 20170516] on linux\n",
      "Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n",
      ">>> "
     ]
    }
   ],
   "source": [
    "\n",
    "!{sys.executable} -m pip install https://storage.googleapis.com/ml-pipeline/release/0.1.17/kfp.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "from kfp import compiler\n",
    "import kfp.dsl as dsl\n",
    "import kfp.gcp as gcp\n",
    "import kfp.notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create image to be used by components. \n",
    "Using tensorflow's py3 image as base and installing requiered libraries (automl, storage, pandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = 'gs://jksandbox/pipelinestest/out'\n",
    "PROJECT_NAME = 'sandbox-235500'\n",
    "EF_IMAGE='gcr.io/%s/automltables:dev' % PROJECT_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-04-26 18:58:52:INFO:Checking path: gs://jksandbox/pipelinestest/out...\n",
      "2019-04-26 18:58:52:INFO:Generate build files.\n",
      "2019-04-26 18:58:52:INFO:Start a kaniko job for build.\n",
      "2019-04-26 18:58:52:INFO:Found local kubernetes config. Initialized with kube_config.\n",
      "2019-04-26 18:58:58:INFO:5 seconds: waiting for job to complete\n",
      "2019-04-26 18:59:03:INFO:10 seconds: waiting for job to complete\n",
      "2019-04-26 18:59:08:INFO:15 seconds: waiting for job to complete\n",
      "2019-04-26 18:59:13:INFO:20 seconds: waiting for job to complete\n",
      "2019-04-26 18:59:18:INFO:25 seconds: waiting for job to complete\n",
      "2019-04-26 18:59:23:INFO:30 seconds: waiting for job to complete\n",
      "2019-04-26 18:59:28:INFO:35 seconds: waiting for job to complete\n",
      "2019-04-26 18:59:33:INFO:40 seconds: waiting for job to complete\n",
      "2019-04-26 18:59:38:INFO:45 seconds: waiting for job to complete\n",
      "2019-04-26 18:59:43:INFO:50 seconds: waiting for job to complete\n",
      "2019-04-26 18:59:48:INFO:55 seconds: waiting for job to complete\n",
      "2019-04-26 18:59:53:INFO:60 seconds: waiting for job to complete\n",
      "2019-04-26 18:59:58:INFO:65 seconds: waiting for job to complete\n",
      "2019-04-26 19:00:03:INFO:70 seconds: waiting for job to complete\n",
      "2019-04-26 19:00:09:INFO:76 seconds: waiting for job to complete\n",
      "2019-04-26 19:00:14:INFO:81 seconds: waiting for job to complete\n",
      "2019-04-26 19:00:19:INFO:86 seconds: waiting for job to complete\n",
      "2019-04-26 19:00:19:INFO:Kaniko job complete.\n",
      "2019-04-26 19:00:19:INFO:Build image complete.\n"
     ]
    }
   ],
   "source": [
    "%%docker {EF_IMAGE} {OUTPUT_DIR}\n",
    "FROM tensorflow/tensorflow:latest-py3\n",
    "RUN pip3 install --upgrade pandas\n",
    "RUN pip3 install --upgrade google-cloud-storage\n",
    "RUN pip3 install --upgrade google-cloud-automl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create reusable components for running different steps in AutoML Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def create_dataset(\n",
    "    project_id: str,\n",
    "    location: str,\n",
    "    display_name: str) -> str:\n",
    "        \n",
    "    from google.cloud import automl_v1beta1\n",
    "    \n",
    "    client = automl_v1beta1.AutoMlClient()\n",
    "    \n",
    "    location_path = client.location_path(project_id, location)\n",
    "    \n",
    "    create_dataset_response = client.create_dataset(\n",
    "        location_path,\n",
    "        {\n",
    "            'display_name': display_name,\n",
    "            'tables_dataset_metadata': {}})\n",
    "    \n",
    "    return(create_dataset_response.name)\n",
    "    \n",
    "compiler.build_python_component(\n",
    "    component_func = create_dataset,\n",
    "    staging_gcs_path = OUTPUT_DIR,\n",
    "    base_image=EF_IMAGE,\n",
    "    target_component_file='component-create-dataset.yaml',\n",
    "    target_image = 'gcr.io/' + PROJECT_NAME + '/component-create-dataset:latest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def import_data(\n",
    "    dataset_name: str,\n",
    "    source: str,\n",
    "    input_uri: str) -> str:\n",
    "        \n",
    "    from google.cloud import automl_v1beta1\n",
    "    \n",
    "    client = automl_v1beta1.AutoMlClient()\n",
    "    \n",
    "    input_config = {\n",
    "        source: {\n",
    "            'input_uri': input_uri\n",
    "        }}\n",
    "    \n",
    "    import_data_response = client.import_data(\n",
    "        dataset_name,\n",
    "        input_config)\n",
    "    \n",
    "    import_data_response.result()\n",
    "    \n",
    "    return(dataset_name)\n",
    "    \n",
    "compiler.build_python_component(\n",
    "    component_func = import_data,\n",
    "    staging_gcs_path = OUTPUT_DIR,\n",
    "    base_image=EF_IMAGE,\n",
    "    target_component_file='component-import-data.yaml',\n",
    "    target_image = 'gcr.io/' + PROJECT_NAME + '/component-import-data:latest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def column_specs(\n",
    "    dataset_name: str,\n",
    "    bucket: str) -> str:\n",
    "        \n",
    "    from google.cloud import automl_v1beta1\n",
    "    import google.cloud.automl_v1beta1.proto.data_types_pb2 as data_types\n",
    "    from google.cloud import storage\n",
    "    import pickle\n",
    "    import json\n",
    "    import pandas as pd\n",
    "    \n",
    "    client = automl_v1beta1.AutoMlClient()\n",
    "    \n",
    "    list_table_specs_response = client.list_table_specs(dataset_name)\n",
    "    table_specs = [s for s in list_table_specs_response]\n",
    "    table_spec_name = table_specs[0].name\n",
    "    list_column_specs_response = client.list_column_specs(table_spec_name)\n",
    "    column_specs = {s.display_name: s for s in list_column_specs_response}\n",
    "    \n",
    "    file_blob = 'tables/column-specs/' + dataset_name + '.csv'\n",
    "    \n",
    "    ui_out = {\n",
    "        'version': 1,\n",
    "        'outputs': [\n",
    "            {\n",
    "                'type': 'table',\n",
    "                'source': 'gs://'+bucket+'/' + file_blob,\n",
    "                'header': ['Column', 'Type'],\n",
    "                'format':'csv'}]}\n",
    "    \n",
    "    column_types  = pd.DataFrame({\n",
    "        'Column':[x for x in column_specs.keys()],\n",
    "        'Type':[data_types.TypeCode.Name(\n",
    "            column_specs[x].data_type.type_code) for x in column_specs.keys()]\n",
    "    })\n",
    "    \n",
    "    column_types.to_csv('data.csv', header=None, index=None)\n",
    "    gcs_bucket = storage.Client().bucket(bucket)\n",
    "    blob = gcs_bucket.blob(file_blob)\n",
    "    blob.upload_from_filename('data.csv')\n",
    "    \n",
    "    \n",
    "    with open(\n",
    "        '/mlpipeline-ui-metadata.json',\n",
    "        'w') as fp:\n",
    "        json.dump(ui_out, fp)\n",
    "    \n",
    "    \n",
    "    with open(\n",
    "        'data.pk',\n",
    "        mode='wb') as fp:\n",
    "        pickle.dump(column_specs, fp)\n",
    "\n",
    "    \n",
    "    file_pickle = 'tables/column-specs/' + dataset_name + '.pk'\n",
    "    \n",
    "    blob = gcs_bucket.blob(file_pickle)\n",
    "    blob.upload_from_filename('data.pk')\n",
    "    \n",
    "    return(file_pickle)\n",
    "    \n",
    "compiler.build_python_component(\n",
    "    component_func = column_specs,\n",
    "    staging_gcs_path = OUTPUT_DIR,\n",
    "    base_image=EF_IMAGE,\n",
    "    target_component_file='component-column-specs.yaml',\n",
    "    target_image = 'gcr.io/' + PROJECT_NAME + '/component-column-specs:latest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def update_column(\n",
    "    bucket: str,\n",
    "    column_specs_file: str,\n",
    "    column_name: str,\n",
    "    column_type: str) -> str:\n",
    "        \n",
    "    from google.cloud import automl_v1beta1\n",
    "    import google.cloud.automl_v1beta1.proto.data_types_pb2 as data_types\n",
    "    from google.cloud import storage\n",
    "    import pickle\n",
    "    import json\n",
    "    import pandas as pd\n",
    "    \n",
    "    gcs_bucket = storage.Client().bucket(bucket)\n",
    "    blob = gcs_bucket.blob(column_specs_file)\n",
    "    \n",
    "    with open(\n",
    "        'specs.pk',\n",
    "        mode='wb')  as fp:\n",
    "        blob.download_to_file(fp)\n",
    "    \n",
    "    with open(\n",
    "        'specs.pk',\n",
    "        mode='rb') as fp:\n",
    "        column_specs = pickle.load(fp)\n",
    "    \n",
    "    client = automl_v1beta1.AutoMlClient()\n",
    "    \n",
    "    \n",
    "    update_column_spec_dict = {\n",
    "        \"name\": column_specs[column_name].name,\n",
    "        \"data_type\": {\n",
    "            \"type_code\": column_type\n",
    "        }\n",
    "    }\n",
    "    column_specs[column_name] = client.update_column_spec(update_column_spec_dict)\n",
    "    \n",
    "    file_blob = 'tables/column-specs/' + column_specs[column_name].name + '.csv'\n",
    "    \n",
    "    ui_out = {\n",
    "        'version': 1,\n",
    "        'outputs': [\n",
    "            {\n",
    "                'type': 'table',\n",
    "                'source': 'gs://'+bucket+'/' + file_blob,\n",
    "                'header': ['Column', 'Type'],\n",
    "                'format':'csv'}]}\n",
    "    \n",
    "    column_types  = pd.DataFrame({\n",
    "        'Column':[x for x in column_specs.keys()],\n",
    "        'Type':[data_types.TypeCode.Name(\n",
    "            column_specs[x].data_type.type_code) for x in column_specs.keys()]\n",
    "    })\n",
    "    \n",
    "    column_types.to_csv('data.csv', header=None, index=None)\n",
    "    blob = gcs_bucket.blob(file_blob)\n",
    "    blob.upload_from_filename('data.csv')\n",
    "    \n",
    "    \n",
    "    with open(\n",
    "        '/mlpipeline-ui-metadata.json',\n",
    "        'w') as fp:\n",
    "        json.dump(ui_out, fp)\n",
    "    \n",
    "    \n",
    "    with open(\n",
    "        'data.pk',\n",
    "        mode='wb') as fp:\n",
    "        pickle.dump(column_specs, fp)\n",
    "\n",
    "    \n",
    "    file_pickle = 'tables/column-specs/' + column_specs[column_name].name + '.pk'\n",
    "    \n",
    "    blob = gcs_bucket.blob(file_pickle)\n",
    "    blob.upload_from_filename('data.pk')\n",
    "    \n",
    "    return(file_pickle)\n",
    "    \n",
    "compiler.build_python_component(\n",
    "    component_func = update_column,\n",
    "    staging_gcs_path = OUTPUT_DIR,\n",
    "    base_image=EF_IMAGE,\n",
    "    target_component_file='component-update-column.yaml',\n",
    "    target_image = 'gcr.io/' + PROJECT_NAME + '/component-update-column:latest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def update_dataset(\n",
    "    dataset_name: str,\n",
    "    bucket: str,\n",
    "    column_specs_file: str,\n",
    "    label_column: str,\n",
    "    split_column: str) -> str:\n",
    "        \n",
    "    from google.cloud import automl_v1beta1\n",
    "    from google.cloud import storage\n",
    "    import pickle\n",
    "    \n",
    "    gcs_bucket = storage.Client().bucket(bucket)\n",
    "    blob = gcs_bucket.blob(column_specs_file)\n",
    "    \n",
    "    with open(\n",
    "        'specs.pk',\n",
    "        mode='wb')  as fp:\n",
    "        blob.download_to_file(fp)\n",
    "    \n",
    "    with open(\n",
    "        'specs.pk',\n",
    "        mode='rb') as fp:\n",
    "        column_specs = pickle.load(fp)\n",
    "    \n",
    "    client = automl_v1beta1.AutoMlClient()\n",
    "    \n",
    "    label_column_spec = column_specs[label_column]\n",
    "    label_column_id = label_column_spec.name.rsplit('/', 1)[-1]\n",
    "\n",
    "    split_column_spec = column_specs[split_column]\n",
    "    split_column_id = split_column_spec.name.rsplit('/', 1)[-1]\n",
    "\n",
    "    update_dataset_dict = {\n",
    "        'name': dataset_name,\n",
    "        'tables_dataset_metadata': {\n",
    "            'target_column_spec_id': label_column_id,\n",
    "            'ml_use_column_spec_id': split_column_id,\n",
    "        }\n",
    "    }\n",
    "    client.update_dataset(update_dataset_dict)\n",
    "    \n",
    "    return(column_specs_file)\n",
    "    \n",
    "compiler.build_python_component(\n",
    "    component_func = update_dataset,\n",
    "    staging_gcs_path = OUTPUT_DIR,\n",
    "    base_image=EF_IMAGE,\n",
    "    target_component_file='component-update-dataset.yaml',\n",
    "    target_image = 'gcr.io/' + PROJECT_NAME + '/component-update-dataset:latest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def create_model(\n",
    "    project_id: str,\n",
    "    location: str,\n",
    "    display_name: str,\n",
    "    train_hours: str,\n",
    "    optimization_objective: str,\n",
    "    columns_to_ignore: str,\n",
    "    dataset_name: str,\n",
    "    bucket: str,\n",
    "    column_specs_file: str,\n",
    "    label_column: str,\n",
    "    split_column: str) -> str:\n",
    "        \n",
    "    from google.cloud import automl_v1beta1\n",
    "    from google.cloud import storage\n",
    "    import pickle\n",
    "    import json\n",
    "    \n",
    "    gcs_bucket = storage.Client().bucket(bucket)\n",
    "    blob = gcs_bucket.blob(column_specs_file)\n",
    "    \n",
    "    with open(\n",
    "        'specs.pk',\n",
    "        mode='wb')  as fp:\n",
    "        blob.download_to_file(fp)\n",
    "    \n",
    "    with open(\n",
    "        'specs.pk',\n",
    "        mode='rb') as fp:\n",
    "        column_specs = pickle.load(fp)\n",
    "    \n",
    "    client = automl_v1beta1.AutoMlClient()\n",
    "    \n",
    "    location_path = client.location_path(project_id, location)\n",
    "    \n",
    "    feat_list = list(column_specs.keys())\n",
    "    feat_list.remove(label_column)\n",
    "    feat_list.remove(split_column)\n",
    "    for c in json.loads(columns_to_ignore):\n",
    "        feat_list.remove(c)\n",
    "\n",
    "    model_dict = {\n",
    "        'display_name': display_name,\n",
    "        'dataset_id': dataset_name.rsplit('/', 1)[-1],\n",
    "        'tables_model_metadata': {\n",
    "          'train_budget_milli_node_hours':int(train_hours) * 1000,\n",
    "          'optimization_objective': optimization_objective,\n",
    "          'target_column_spec': column_specs[label_column],\n",
    "          'input_feature_column_specs': [\n",
    "                column_specs[x] for x in feat_list]}\n",
    "        }\n",
    "\n",
    "    create_model_response = client.create_model(location_path, model_dict)\n",
    "    create_model_result = create_model_response.result()\n",
    "    return(create_model_result.name)\n",
    "\n",
    "\n",
    "compiler.build_python_component(\n",
    "    component_func = create_model,\n",
    "    staging_gcs_path = OUTPUT_DIR,\n",
    "    base_image=EF_IMAGE,\n",
    "    target_component_file='component-create-model.yaml',\n",
    "    target_image = 'gcr.io/' + PROJECT_NAME + '/component-create-model:latest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def evaluate_model(\n",
    "    model_name: str,\n",
    "    bucket: str) -> str:\n",
    "        \n",
    "    from google.cloud import automl_v1beta1\n",
    "    from google.cloud import storage\n",
    "    import pandas as pd\n",
    "    import json\n",
    "    \n",
    "    client = automl_v1beta1.AutoMlClient()\n",
    "    \n",
    "    \n",
    "    file_blob = 'tables/evaluate-model/' + model_name + '.csv'\n",
    "\n",
    "    ui_out = {\n",
    "        'version': 1,\n",
    "        'outputs': [\n",
    "            {\n",
    "                'type': 'table',\n",
    "                'source': 'gs://'+bucket+'/' + file_blob,\n",
    "                'header': ['Feature', 'Importance'],\n",
    "                'format':'csv'}]}\n",
    "\n",
    "    model = client.get_model(model_name)\n",
    "    feature_list = [(\n",
    "        x.feature_importance,\n",
    "        x.column_display_name\n",
    "    ) for x in model.tables_model_metadata.tables_model_column_info]\n",
    "\n",
    "    feature_list.sort(reverse=True)\n",
    "\n",
    "\n",
    "    feature_importance  = pd.DataFrame({\n",
    "        'Feature':[x[1] for x in feature_list],\n",
    "        'Importance':[x[0] for x in feature_list]})\n",
    "\n",
    "    feature_importance.to_csv('data.csv', header=None, index=None)\n",
    "    gcs_bucket = storage.Client().bucket(bucket)\n",
    "    blob = gcs_bucket.blob(file_blob)\n",
    "    blob.upload_from_filename('data.csv')\n",
    "\n",
    "\n",
    "    with open(\n",
    "        '/mlpipeline-ui-metadata.json',\n",
    "        'w') as fp:\n",
    "        json.dump(ui_out, fp)\n",
    "    \n",
    "    \n",
    "    \n",
    "    metrics = [x for x in client.list_model_evaluations(model_name)][-1]\n",
    "    return(metrics.regression_evaluation_metrics)\n",
    "    \n",
    "compiler.build_python_component(\n",
    "    component_func = evaluate_model,\n",
    "    staging_gcs_path = OUTPUT_DIR,\n",
    "    base_image=EF_IMAGE,\n",
    "    target_component_file='component-evaluate-model.yaml',\n",
    "    target_image = 'gcr.io/' + PROJECT_NAME + '/component-evaluate-model:latest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample pipeline (Energy Price Forecasting) that uses the AutoML Tables components to build a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name='AutoML Tables',\n",
    "    description='AutoML Tables Pipeline')\n",
    "def automl_tables_pipeline(\n",
    "    project_id = dsl.PipelineParam(\n",
    "        'project_id',\n",
    "        value='energy-forecasting'),\n",
    "    location = dsl.PipelineParam(\n",
    "        'location',\n",
    "        value='us-central1'),\n",
    "    bucket = dsl.PipelineParam(\n",
    "        'bucket',\n",
    "        value='energy-forecasting'),\n",
    "    dataset_display_name = dsl.PipelineParam(\n",
    "        'dataset_display_name',\n",
    "        value='testdataset'),\n",
    "    data_source = dsl.PipelineParam(\n",
    "        'data_source',\n",
    "        value='bigquery_source'),\n",
    "    data_input_uri = dsl.PipelineParam(\n",
    "        'data_input_uri',\n",
    "        value='bq://energy-forecasting.Energy.automldata'),\n",
    "    column_to_update_name = dsl.PipelineParam(\n",
    "        'column_to_update_name',\n",
    "        value='hour'),\n",
    "    column_to_update_type = dsl.PipelineParam(\n",
    "        'column_to_update_type',\n",
    "        value='CATEGORY'),\n",
    "    label_column = dsl.PipelineParam(\n",
    "        'label_column',\n",
    "        value='price'),\n",
    "    split_column = dsl.PipelineParam(\n",
    "        'split_column',\n",
    "        value='split'),\n",
    "    model_display_name = dsl.PipelineParam(\n",
    "        'model_display_name',\n",
    "        value='testmodel'),\n",
    "    model_train_hours = dsl.PipelineParam(\n",
    "        'model_train_hours',\n",
    "        value='1'),\n",
    "    model_optimization_objective = dsl.PipelineParam(\n",
    "        'model_optimization_objective',\n",
    "        value='MINIMIZE_MAE'),\n",
    "    model_columns_to_ignore = dsl.PipelineParam(\n",
    "        'model_columns_to_ignore',\n",
    "        value='[\"date_utc\"]'),\n",
    "):\n",
    "    \n",
    "    CreateDatasetOp = kfp.components.load_component('component-create-dataset.yaml')\n",
    "    ImportDataOp = kfp.components.load_component('component-import-data.yaml')\n",
    "    ColumnSpecsOp = kfp.components.load_component('component-column-specs.yaml')\n",
    "    UpdateColumnOp = kfp.components.load_component('component-update-column.yaml')\n",
    "    UpdateDatasetOp = kfp.components.load_component('component-update-dataset.yaml')\n",
    "    CreateModelOp = kfp.components.load_component('component-create-model.yaml')\n",
    "    EvaluateModelOp = kfp.components.load_component('component-evaluate-model.yaml')\n",
    "    \n",
    "    cd_op = CreateDatasetOp(\n",
    "        project_id,\n",
    "        location,\n",
    "        dataset_display_name).apply(\n",
    "        gcp.use_gcp_secret('user-gcp-sa'))\n",
    "    \n",
    "    id_op = ImportDataOp(\n",
    "        cd_op.output,\n",
    "        data_source,\n",
    "        data_input_uri).apply(\n",
    "        gcp.use_gcp_secret('user-gcp-sa'))\n",
    "    \n",
    "    cs_op = ColumnSpecsOp(\n",
    "        id_op.output,\n",
    "        bucket).apply(\n",
    "        gcp.use_gcp_secret('user-gcp-sa'))\n",
    "    \n",
    "    uc_op = UpdateColumnOp(\n",
    "        bucket,\n",
    "        cs_op.output,\n",
    "        column_to_update_name,\n",
    "        column_to_update_type).apply(\n",
    "        gcp.use_gcp_secret('user-gcp-sa'))\n",
    "    \n",
    "    ud_op = UpdateDatasetOp(\n",
    "        id_op.output,\n",
    "        bucket,\n",
    "        uc_op.output,\n",
    "        label_column,\n",
    "        split_column).apply(\n",
    "        gcp.use_gcp_secret('user-gcp-sa'))\n",
    "    \n",
    "    cm_op = CreateModelOp(\n",
    "        project_id,\n",
    "        location,\n",
    "        model_display_name,\n",
    "        model_train_hours,\n",
    "        model_optimization_objective,\n",
    "        model_columns_to_ignore,\n",
    "        id_op.output,\n",
    "        bucket,\n",
    "        ud_op.output,\n",
    "        label_column,\n",
    "        split_column).apply(\n",
    "        gcp.use_gcp_secret('user-gcp-sa'))\n",
    "    \n",
    "    em_op = EvaluateModelOp(\n",
    "        cm_op.output,\n",
    "        bucket).apply(\n",
    "        gcp.use_gcp_secret('user-gcp-sa'))\n",
    "    \n",
    "    \n",
    "compiler.Compiler().compile(automl_tables_pipeline, 'automl-tables-pipeline.tar.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "KFP",
   "language": "python",
   "name": "kfp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
